[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDouble2IntMap$2$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDouble2IntMap$2.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDouble2IntMap$BasicEntry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDouble2IntMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDouble2IntSortedMap$KeySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDouble2IntSortedMap$KeySetIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDouble2IntSortedMap$ValuesCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDouble2IntSortedMap$ValuesIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDouble2IntSortedMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDoubleBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDoubleCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDoubleIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDoubleList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDoubleList$DoubleSubList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDoubleList$DoubleSubList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDoubleList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDoubleListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDoubleSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/AbstractDoubleSortedSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntFunction.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntLinkedOpenHashMap$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntLinkedOpenHashMap$EntryIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntLinkedOpenHashMap$FastEntryIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntLinkedOpenHashMap$KeyIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntLinkedOpenHashMap$KeySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntLinkedOpenHashMap$MapEntry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntLinkedOpenHashMap$MapEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntLinkedOpenHashMap$MapIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntLinkedOpenHashMap$ValueIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntLinkedOpenHashMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntMap$Entry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntMap$FastEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntSortedMap$FastSortedEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/Double2IntSortedMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleArrayList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleArrayList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleArrays$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleArrays$ArrayHashStrategy.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleArrays.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleComparator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterable.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$ArrayIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$ByteIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$EmptyIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$FloatIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$IntIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$IteratorConcatenator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$IteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$ListIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$ShortIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$SingletonIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$UnmodifiableBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$UnmodifiableIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators$UnmodifiableListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleIterators.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleSortedSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/doubles/DoubleStack.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloat2IntFunction.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloat2IntMap$1$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloat2IntMap$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloat2IntMap$2$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloat2IntMap$2.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloat2IntMap$BasicEntry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloat2IntMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloat2IntSortedMap$KeySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloat2IntSortedMap$KeySetIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloat2IntSortedMap$ValuesCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloat2IntSortedMap$ValuesIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloat2IntSortedMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloatBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloatCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloatIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloatList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloatList$FloatSubList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloatList$FloatSubList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloatList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloatListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloatSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/AbstractFloatSortedSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntFunction.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntLinkedOpenHashMap$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntLinkedOpenHashMap$EntryIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntLinkedOpenHashMap$FastEntryIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntLinkedOpenHashMap$KeyIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntLinkedOpenHashMap$KeySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntLinkedOpenHashMap$MapEntry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntLinkedOpenHashMap$MapEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntLinkedOpenHashMap$MapIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntLinkedOpenHashMap$ValueIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntLinkedOpenHashMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntMap$Entry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntMap$FastEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntSortedMap$FastSortedEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/Float2IntSortedMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatArrayList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatArrayList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatArrays$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatArrays$ArrayHashStrategy.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatArrays.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatComparator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterable.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterators$ArrayIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterators$ByteIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterators$EmptyIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterators$IteratorConcatenator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterators$IteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterators$ListIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterators$ShortIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterators$SingletonIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterators$UnmodifiableBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterators$UnmodifiableIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterators$UnmodifiableListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatIterators.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatSortedSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/floats/FloatStack.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractInt2IntFunction.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractInt2IntMap$1$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractInt2IntMap$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractInt2IntMap$2$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractInt2IntMap$2.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractInt2IntMap$BasicEntry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractInt2IntMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractInt2IntSortedMap$KeySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractInt2IntSortedMap$KeySetIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractInt2IntSortedMap$ValuesCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractInt2IntSortedMap$ValuesIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractInt2IntSortedMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractIntBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractIntCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractIntIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractIntList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractIntList$IntSubList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractIntList$IntSubList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractIntList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractIntListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractIntSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/AbstractIntSortedSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntFunction.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntLinkedOpenHashMap$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntLinkedOpenHashMap$EntryIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntLinkedOpenHashMap$FastEntryIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntLinkedOpenHashMap$KeyIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntLinkedOpenHashMap$KeySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntLinkedOpenHashMap$MapEntry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntLinkedOpenHashMap$MapEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntLinkedOpenHashMap$MapIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntLinkedOpenHashMap$ValueIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntLinkedOpenHashMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntMap$Entry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntMap$FastEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntSortedMap$FastSortedEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/Int2IntSortedMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntArrayList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntArrayList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntArrays$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntArrays$ArrayHashStrategy.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntArrays.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntComparator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterable.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators$ArrayIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators$ByteIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators$EmptyIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators$IntervalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators$IteratorConcatenator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators$IteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators$ListIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators$ShortIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators$SingletonIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators$UnmodifiableBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators$UnmodifiableIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators$UnmodifiableListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntIterators.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntSortedSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/ints/IntStack.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLong2IntFunction.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLong2IntMap$1$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLong2IntMap$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLong2IntMap$2$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLong2IntMap$2.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLong2IntMap$BasicEntry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLong2IntMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLong2IntSortedMap$KeySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLong2IntSortedMap$KeySetIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLong2IntSortedMap$ValuesCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLong2IntSortedMap$ValuesIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLong2IntSortedMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLongBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLongCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLongIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLongList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLongList$LongSubList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLongList$LongSubList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLongList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLongListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLongSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/AbstractLongSortedSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntFunction.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntLinkedOpenHashMap$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntLinkedOpenHashMap$EntryIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntLinkedOpenHashMap$FastEntryIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntLinkedOpenHashMap$KeyIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntLinkedOpenHashMap$KeySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntLinkedOpenHashMap$MapEntry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntLinkedOpenHashMap$MapEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntLinkedOpenHashMap$MapIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntLinkedOpenHashMap$ValueIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntLinkedOpenHashMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntMap$Entry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntMap$FastEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntSortedMap$FastSortedEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/Long2IntSortedMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongArrayList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongArrayList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongArrays$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongArrays$ArrayHashStrategy.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongArrays.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongComparator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterable.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$ArrayIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$ByteIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$EmptyIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$IntIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$IntervalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$IteratorConcatenator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$IteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$ListIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$ShortIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$SingletonIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$UnmodifiableBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$UnmodifiableIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators$UnmodifiableListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongIterators.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongSortedSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/longs/LongStack.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObject2IntFunction.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObject2IntMap$1$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObject2IntMap$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObject2IntMap$2$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObject2IntMap$2.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObject2IntMap$BasicEntry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObject2IntMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObject2IntSortedMap$KeySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObject2IntSortedMap$KeySetIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObject2IntSortedMap$ValuesCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObject2IntSortedMap$ValuesIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObject2IntSortedMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObjectBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObjectCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObjectIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObjectList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObjectList$ObjectSubList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObjectList$ObjectSubList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObjectList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObjectListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObjectSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/AbstractObjectSortedSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntFunction.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntLinkedOpenHashMap$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntLinkedOpenHashMap$EntryIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntLinkedOpenHashMap$FastEntryIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntLinkedOpenHashMap$KeyIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntLinkedOpenHashMap$KeySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntLinkedOpenHashMap$MapEntry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntLinkedOpenHashMap$MapEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntLinkedOpenHashMap$MapIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntLinkedOpenHashMap$ValueIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntLinkedOpenHashMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntMap$Entry.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntMap$FastEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntSortedMap$FastSortedEntrySet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/Object2IntSortedMap.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectArrayList$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectArrayList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectArrays$1.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectArrays$ArrayHashStrategy.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectArrays.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectCollection.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectIterable.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectIterators$ArrayIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectIterators$EmptyIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectIterators$IteratorConcatenator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectIterators$IteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectIterators$ListIteratorWrapper.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectIterators$SingletonIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectIterators$UnmodifiableBidirectionalIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectIterators$UnmodifiableIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectIterators$UnmodifiableListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectIterators.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectList.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectListIterator.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/objects/ObjectSortedSet.class' with strategy 'first'
[warn] Merging 'parquet/it/unimi/dsi/fastutil/shorts/ShortIterator.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/EncodingUtils.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/ProcessFunction.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/ShortStack.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TApplicationException.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TBase.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TBaseHelper$1.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TBaseHelper$NestedStructureComparator.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TBaseHelper.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TBaseProcessor.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TByteArrayOutputStream.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TEnum.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TEnumHelper.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TException.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TFieldIdEnum.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TFieldRequirementType.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TProcessor.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TProcessorFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TServiceClient.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TServiceClientFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/TUnion.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/async/AsyncMethodCallback.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/async/TAsyncClient.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/async/TAsyncClientFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/async/TAsyncClientManager$1.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/async/TAsyncClientManager$SelectThread.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/async/TAsyncClientManager$TAsyncMethodCallTimeoutComparator.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/async/TAsyncClientManager.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/async/TAsyncMethodCall$1.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/async/TAsyncMethodCall$State.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/async/TAsyncMethodCall.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/meta_data/EnumMetaData.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/meta_data/FieldMetaData.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/meta_data/FieldValueMetaData.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/meta_data/ListMetaData.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/meta_data/MapMetaData.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/meta_data/SetMetaData.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/meta_data/StructMetaData.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TBase64Utils.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TBinaryProtocol$Factory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TBinaryProtocol.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TCompactProtocol$Factory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TCompactProtocol$Types.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TCompactProtocol.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TField.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TJSONProtocol$Factory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TJSONProtocol$JSONBaseContext.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TJSONProtocol$JSONListContext.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TJSONProtocol$JSONPairContext.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TJSONProtocol$LookaheadReader.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TJSONProtocol.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TList.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TMap.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TMessage.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TMessageType.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TProtocol.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TProtocolException.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TProtocolFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TProtocolUtil.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TSet.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TSimpleJSONProtocol$Context.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TSimpleJSONProtocol$Factory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TSimpleJSONProtocol$ListContext.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TSimpleJSONProtocol$StructContext.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TSimpleJSONProtocol.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TStruct.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/protocol/TType.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/THsHaServer$Args.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/THsHaServer$Invocation.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/THsHaServer.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TNonblockingServer$AbstractNonblockingServerArgs.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TNonblockingServer$Args.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TNonblockingServer$FrameBuffer.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TNonblockingServer$SelectThread.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TNonblockingServer.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TServer$AbstractServerArgs.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TServer$Args.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TServer.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TServlet$1.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TServlet.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TSimpleServer.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TThreadPoolServer$1.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TThreadPoolServer$Args.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TThreadPoolServer$WorkerProcess.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/server/TThreadPoolServer.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/AutoExpandingBuffer.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/AutoExpandingBufferReadTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/AutoExpandingBufferWriteTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TFastFramedTransport$Factory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TFastFramedTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TFileProcessor.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TFileTransport$Event.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TFileTransport$chunkState.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TFileTransport$tailPolicy.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TFileTransport$truncableBufferedInputStream.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TFileTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TFramedTransport$Factory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TFramedTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/THttpClient$Factory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/THttpClient.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TIOStreamTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TMemoryBuffer.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TMemoryInputTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TNonblockingServerSocket.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TNonblockingServerTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TNonblockingSocket.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TNonblockingTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSSLTransportFactory$TSSLTransportParameters.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSSLTransportFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSaslClientTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSaslServerTransport$1.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSaslServerTransport$Factory.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSaslServerTransport$TSaslServerDefinition.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSaslServerTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSaslTransport$NegotiationStatus.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSaslTransport$SaslParticipant.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSaslTransport$SaslResponse.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSaslTransport$SaslRole.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSaslTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSeekableFile.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TServerSocket.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TServerTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TSocket.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TStandardFile.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TTransport.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TTransportException.class' with strategy 'first'
[warn] Merging 'parquet/org/apache/thrift/transport/TTransportFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/Base64Variant.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/Base64Variants.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/FormatSchema.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonEncoding.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonGenerationException.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonGenerator$Feature.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonGenerator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonLocation.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonParseException.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonParser$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonParser$Feature.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonParser$NumberType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonParser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonProcessingException.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonStreamContext.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/JsonToken.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/ObjectCodec.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/PrettyPrinter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/SerializableString.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/Version.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/Versioned.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JacksonAnnotation.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonAnyGetter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonAnySetter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonAutoDetect$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonAutoDetect$Visibility.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonAutoDetect.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonBackReference.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonCreator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonGetter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonIgnore.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonIgnoreProperties.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonIgnoreType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonManagedReference.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonMethod.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonProperty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonPropertyOrder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonRawValue.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonSetter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonSubTypes$Type.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonSubTypes.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonTypeInfo$As.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonTypeInfo$Id.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonTypeInfo$None.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonTypeInfo.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonTypeName.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonUnwrapped.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonValue.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/annotate/JsonWriteNullProperties.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/format/DataFormatDetector.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/format/DataFormatMatcher.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/format/InputAccessor$Std.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/format/InputAccessor.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/format/MatchStrength.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/ByteSourceBootstrapper$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/ByteSourceBootstrapper.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/DefaultPrettyPrinter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/Indenter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/JsonGeneratorBase$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/JsonGeneratorBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/JsonNumericParserBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/JsonParserBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/JsonParserMinimalBase$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/JsonParserMinimalBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/JsonReadContext.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/JsonWriteContext.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/ReaderBasedParser$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/ReaderBasedParser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/ReaderBasedParserBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/StreamBasedParserBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/Utf8Generator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/Utf8StreamParser$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/Utf8StreamParser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/VERSION.txt' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/impl/WriterBasedGenerator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/BaseReader.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/CharacterEscapes.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/IOContext.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/InputDecorator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/JsonStringEncoder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/MergedStream.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/NumberInput.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/NumberOutput.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/OutputDecorator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/SegmentedStringWriter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/SerializedString.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/UTF32Reader.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/io/UTF8Writer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/AbstractTypeResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/AnnotationIntrospector$Pair.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/AnnotationIntrospector$ReferenceProperty$Type.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/AnnotationIntrospector$ReferenceProperty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/AnnotationIntrospector.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/BeanDescription.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/BeanProperty$Std.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/BeanProperty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/BeanPropertyDefinition.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ClassIntrospector$MixInResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ClassIntrospector.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ContextualDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ContextualKeyDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ContextualSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/DeserializationConfig$Feature.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/DeserializationConfig.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/DeserializationContext.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/DeserializationProblemHandler.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/DeserializerFactory$Config.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/DeserializerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/DeserializerProvider.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/Deserializers$Base.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/Deserializers$None.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/Deserializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/HandlerInstantiator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/InjectableValues$Std.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/InjectableValues.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/JsonDeserializer$None.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/JsonDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/JsonMappingException$Reference.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/JsonMappingException.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/JsonSerializable.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/JsonSerializableWithType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/JsonSerializer$None.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/JsonSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/KeyDeserializer$None.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/KeyDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/KeyDeserializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/MapperConfig$Base.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/MapperConfig$ConfigFeature.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/MapperConfig$Impl.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/MapperConfig.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/MappingIterator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/MappingJsonFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/Module$SetupContext.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/Module.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ObjectMapper$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ObjectMapper$2.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ObjectMapper$DefaultTypeResolverBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ObjectMapper$DefaultTyping.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ObjectMapper.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ObjectReader.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ObjectWriter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/PropertyNamingStrategy$LowerCaseWithUnderscoresStrategy.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/PropertyNamingStrategy$PropertyNamingStrategyBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/PropertyNamingStrategy.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ResolvableDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ResolvableSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/RuntimeJsonMappingException.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/SerializationConfig$Feature.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/SerializationConfig.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/SerializerFactory$Config.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/SerializerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/SerializerProvider.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/Serializers$Base.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/Serializers$None.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/Serializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/TypeDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/TypeSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/VERSION.txt' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JacksonInject.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JacksonStdImpl.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JsonCachable.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JsonDeserialize.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JsonFilter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JsonRootName.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JsonSerialize$Inclusion.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JsonSerialize$Typing.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JsonSerialize.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JsonTypeIdResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JsonTypeResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JsonValueInstantiator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/JsonView.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/annotate/NoClass.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/AbstractDeserializer$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/AbstractDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/ArrayDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/ArrayDeserializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/BasicDeserializerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/BeanDeserializer$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/BeanDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/BeanDeserializerBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/BeanDeserializerFactory$ConfigImpl.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/BeanDeserializerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/BeanDeserializerModifier.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/CollectionDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/ContainerDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/CustomDeserializerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/DateDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/EnumDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/EnumResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/FromStringDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/JsonNodeDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/MapDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/SettableAnyProperty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/SettableBeanProperty$FieldProperty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/SettableBeanProperty$InnerClassProperty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/SettableBeanProperty$ManagedReferenceProperty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/SettableBeanProperty$MethodProperty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/SettableBeanProperty$NullProvider.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/SettableBeanProperty$SetterlessProperty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/SettableBeanProperty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/StdDeserializationContext.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/StdDeserializer$CalendarDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/StdDeserializer$ClassDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/StdDeserializer$StringDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/StdDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/StdDeserializerProvider$WrappedDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/StdDeserializerProvider.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/StdDeserializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/StdKeyDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/StdKeyDeserializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/StdScalarDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/ThrowableDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/UntypedObjectDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/ValueInstantiator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/ValueInstantiators$Base.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/ValueInstantiators.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/BeanPropertyMap$Bucket.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/BeanPropertyMap$IteratorImpl.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/BeanPropertyMap.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/CreatorCollector.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/CreatorProperty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/ExternalTypeHandler$Builder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/ExternalTypeHandler$ExtTypedProperty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/ExternalTypeHandler.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/PropertyBasedCreator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/PropertyValue$Any.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/PropertyValue$Map.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/PropertyValue$Regular.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/PropertyValue.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/PropertyValueBuffer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/UnwrappedPropertyHandler.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/impl/ValueInjector.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/AtomicBooleanDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/AtomicReferenceDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/BaseNodeDeserializer$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/BaseNodeDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/CalendarDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/ClassDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/CollectionDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/ContainerDeserializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/DateDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/EnumDeserializer$FactoryBasedDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/EnumDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/EnumMapDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/EnumSetDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/FromStringDeserializer$CharsetDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/FromStringDeserializer$CurrencyDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/FromStringDeserializer$InetAddressDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/FromStringDeserializer$LocaleDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/FromStringDeserializer$PatternDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/FromStringDeserializer$TimeZoneDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/FromStringDeserializer$URIDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/FromStringDeserializer$URLDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/FromStringDeserializer$UUIDDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/FromStringDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/JavaTypeDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/JsonNodeDeserializer$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/JsonNodeDeserializer$ArrayDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/JsonNodeDeserializer$ObjectDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/JsonNodeDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/MapDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/ObjectArrayDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/PrimitiveArrayDeserializers$Base.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/PrimitiveArrayDeserializers$BooleanDeser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/PrimitiveArrayDeserializers$ByteDeser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/PrimitiveArrayDeserializers$CharDeser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/PrimitiveArrayDeserializers$DoubleDeser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/PrimitiveArrayDeserializers$FloatDeser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/PrimitiveArrayDeserializers$IntDeser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/PrimitiveArrayDeserializers$LongDeser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/PrimitiveArrayDeserializers$ShortDeser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/PrimitiveArrayDeserializers$StringDeser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/PrimitiveArrayDeserializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$BigDecimalDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$BigIntegerDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$BooleanDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$ByteDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$CharacterDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$DoubleDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$FloatDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$IntegerDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$LongDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$NumberDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$PrimitiveOrWrapperDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$ShortDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$SqlDateDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer$StackTraceElementDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$BoolKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$ByteKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$CalendarKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$CharKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$DateKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$DoubleKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$EnumKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$FloatKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$IntKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$LongKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$ShortKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$StringCtorKeyDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$StringFactoryKeyDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$StringKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer$UuidKD.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdKeyDeserializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdScalarDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StdValueInstantiator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StringCollectionDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/StringDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/ThrowableDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/TimestampDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/TokenBufferDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/UntypedObjectDeserializer$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/deser/std/UntypedObjectDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/exc/UnrecognizedPropertyException.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/CoreXMLDeserializers$DurationDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/CoreXMLDeserializers$GregorianCalendarDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/CoreXMLDeserializers$QNameDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/CoreXMLDeserializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/CoreXMLSerializers$XMLGregorianCalendarSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/CoreXMLSerializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/DOMDeserializer$DocumentDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/DOMDeserializer$NodeDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/DOMDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/DOMSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaDeserializers$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaDeserializers$DateMidnightDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaDeserializers$DateTimeDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaDeserializers$JodaDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaDeserializers$LocalDateDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaDeserializers$LocalDateTimeDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaDeserializers$PeriodDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaDeserializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaSerializers$DateMidnightSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaSerializers$DateTimeSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaSerializers$JodaSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaSerializers$LocalDateSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaSerializers$LocalDateTimeSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/JodaSerializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ext/OptionalHandlerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/Annotated.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/AnnotatedClass.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/AnnotatedConstructor.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/AnnotatedField.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/AnnotatedMember.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/AnnotatedMethod.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/AnnotatedMethodMap.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/AnnotatedParameter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/AnnotatedWithParams.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/AnnotationMap.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/BasicBeanDescription.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/BasicClassIntrospector$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/BasicClassIntrospector$GetterMethodFilter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/BasicClassIntrospector$MinimalMethodFilter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/BasicClassIntrospector$SetterAndGetterMethodFilter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/BasicClassIntrospector$SetterMethodFilter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/BasicClassIntrospector.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/JacksonAnnotationIntrospector.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/MemberKey.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/MethodFilter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/NopAnnotationIntrospector.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/POJOPropertiesCollector.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/POJOPropertyBuilder$Node.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/POJOPropertyBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/VisibilityChecker$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/VisibilityChecker$Std.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/introspect/VisibilityChecker.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/NamedType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/SubtypeResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/TypeIdResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/TypeResolverBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/AsArrayTypeDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/AsArrayTypeSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/AsExternalTypeDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/AsExternalTypeSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/AsPropertyTypeDeserializer$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/AsPropertyTypeDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/AsPropertyTypeSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/AsWrapperTypeDeserializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/AsWrapperTypeSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/ClassNameIdResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/MinimalClassNameIdResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/StdSubtypeResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/StdTypeResolverBuilder$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/StdTypeResolverBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/TypeDeserializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/TypeIdResolverBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/TypeNameIdResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/jsontype/impl/TypeSerializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/module/SimpleAbstractTypeResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/module/SimpleDeserializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/module/SimpleKeyDeserializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/module/SimpleModule.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/module/SimpleSerializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/module/SimpleValueInstantiators.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/AnyGetterWriter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/ArraySerializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/BasicSerializerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/BeanPropertyFilter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/BeanPropertyWriter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/BeanSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/BeanSerializerBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/BeanSerializerFactory$ConfigImpl.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/BeanSerializerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/BeanSerializerModifier.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/ContainerSerializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/CustomSerializerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/EnumSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/FilterProvider.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/FilteredBeanPropertyWriter$MultiView.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/FilteredBeanPropertyWriter$SingleView.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/FilteredBeanPropertyWriter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/JdkSerializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/MapSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/PropertyBuilder$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/PropertyBuilder$EmptyArrayChecker.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/PropertyBuilder$EmptyCollectionChecker.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/PropertyBuilder$EmptyMapChecker.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/PropertyBuilder$EmptyStringChecker.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/PropertyBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/ScalarSerializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/SerializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdKeySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializerProvider$WrappedSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializerProvider.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$BooleanSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$CalendarSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$DoubleSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$FloatSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$IntLikeSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$IntegerSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$LongSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$NumberSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$SerializableSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$SerializableWithTypeSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$SqlDateSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$SqlTimeSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$StringSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers$UtilDateSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/StdSerializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/ToStringSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/FailingSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/JsonSerializerMap$Bucket.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/JsonSerializerMap.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/PropertySerializerMap$Double.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/PropertySerializerMap$Empty.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/PropertySerializerMap$Multi.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/PropertySerializerMap$SerializerAndMapResult.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/PropertySerializerMap$Single.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/PropertySerializerMap$TypeAndSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/PropertySerializerMap.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/ReadOnlyClassToSerializerMap.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/SerializerCache$TypeKey.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/SerializerCache.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/SimpleBeanPropertyFilter$FilterExceptFilter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/SimpleBeanPropertyFilter$SerializeExceptFilter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/SimpleBeanPropertyFilter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/SimpleFilterProvider.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/UnknownSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/UnwrappingBeanPropertyWriter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/impl/UnwrappingBeanSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/AsArraySerializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/BeanSerializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/CalendarSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/CollectionSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/ContainerSerializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/DateSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/EnumMapSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/EnumSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/EnumSetSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/IndexedStringListSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/InetAddressSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/IterableSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/JsonValueSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/MapSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/NonTypedScalarSerializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/NullSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/ObjectArraySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/RawSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/ScalarSerializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/SerializableSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/SerializableWithTypeSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/SerializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StaticListSerializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdArraySerializers$ArraySerializerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdArraySerializers$BooleanArraySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdArraySerializers$ByteArraySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdArraySerializers$CharArraySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdArraySerializers$DoubleArraySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdArraySerializers$FloatArraySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdArraySerializers$IntArraySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdArraySerializers$LongArraySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdArraySerializers$ShortArraySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdArraySerializers$StringArraySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdArraySerializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdContainerSerializers$IndexedListSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdContainerSerializers$IteratorSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdContainerSerializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdJdkSerializers$AtomicBooleanSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdJdkSerializers$AtomicIntegerSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdJdkSerializers$AtomicLongSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdJdkSerializers$AtomicReferenceSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdJdkSerializers$ClassSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdJdkSerializers$FileSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdJdkSerializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdKeySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdKeySerializers$CalendarKeySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdKeySerializers$DateKeySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdKeySerializers$StringKeySerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StdKeySerializers.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StringCollectionSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/StringSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/TimeZoneSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/ToStringSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/ser/std/TokenBufferSerializer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/ArrayType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/ClassKey.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/CollectionLikeType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/CollectionType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/HierarchicType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/MapLikeType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/MapType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/SimpleType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/TypeBase.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/TypeBindings.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/TypeFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/TypeModifier.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/TypeParser$MyTokenizer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/type/TypeParser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/Annotations.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ArrayBuilders$ArrayIterator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ArrayBuilders$BooleanBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ArrayBuilders$ByteBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ArrayBuilders$DoubleBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ArrayBuilders$FloatBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ArrayBuilders$IntBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ArrayBuilders$LongBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ArrayBuilders$ShortBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ArrayBuilders.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/BeanUtil.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ClassUtil$EnumTypeLocator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ClassUtil.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/Comparators$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/Comparators.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/EnumResolver.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/EnumValues.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ISO8601DateFormat.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ISO8601Utils.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/JSONPObject.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/JSONWrappedObject.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/LRUMap.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/LinkedNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/Named.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ObjectBuffer$Node.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/ObjectBuffer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/PrimitiveArrayBuilder$Node.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/PrimitiveArrayBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/Provider.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/RootNameLookup.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/map/util/StdDateFormat.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/ArrayNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/BaseJsonNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/BigIntegerNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/BinaryNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/BooleanNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/ContainerNode$NoNodesIterator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/ContainerNode$NoStringsIterator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/ContainerNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/DecimalNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/DoubleNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/IntNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/JsonNodeFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/LongNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/MissingNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/NodeCursor$Array.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/NodeCursor$Object.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/NodeCursor$RootValue.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/NodeCursor.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/NullNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/NumericNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/ObjectNode$NoFieldsIterator.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/ObjectNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/POJONode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/TextNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/TreeTraversingParser$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/TreeTraversingParser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/node/ValueNode.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/schema/JsonSchema.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/schema/JsonSerializableSchema.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/schema/SchemaAware.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/sym/BytesToNameCanonicalizer$Bucket.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/sym/BytesToNameCanonicalizer$TableInfo.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/sym/BytesToNameCanonicalizer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/sym/CharsToNameCanonicalizer$Bucket.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/sym/CharsToNameCanonicalizer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/sym/Name.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/sym/Name1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/sym/Name2.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/sym/Name3.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/sym/NameN.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/type/JavaType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/type/TypeReference.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/BufferRecycler$ByteBufferType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/BufferRecycler$CharBufferType.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/BufferRecycler.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/ByteArrayBuilder.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/CharTypes.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/DefaultPrettyPrinter$FixedSpaceIndenter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/DefaultPrettyPrinter$Lf2SpacesIndenter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/DefaultPrettyPrinter$NopIndenter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/DefaultPrettyPrinter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/InternCache.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/JsonGeneratorDelegate.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/JsonParserDelegate.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/JsonParserSequence.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/MinimalPrettyPrinter.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/TextBuffer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/TokenBuffer$1.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/TokenBuffer$Parser.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/TokenBuffer$Segment.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/TokenBuffer.class' with strategy 'first'
[warn] Merging 'parquet/org/codehaus/jackson/util/VersionUtil.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/ILoggerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/IMarkerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/Logger.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/LoggerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/MDC.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/Marker.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/MarkerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/helpers/BasicMDCAdapter.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/helpers/BasicMarker.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/helpers/BasicMarkerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/helpers/FormattingTuple.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/helpers/MarkerIgnoringBase.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/helpers/MessageFormatter.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/helpers/NOPLogger.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/helpers/NOPLoggerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/helpers/NOPMDCAdapter.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/helpers/NamedLoggerBase.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/helpers/SubstituteLoggerFactory.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/helpers/Util.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/spi/LocationAwareLogger.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/spi/LoggerFactoryBinder.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/spi/MDCAdapter.class' with strategy 'first'
[warn] Merging 'parquet/org/slf4j/spi/MarkerFactoryBinder.class' with strategy 'first'
[warn] Merging 'parquet/schema/ConversionPatterns.class' with strategy 'first'
[warn] Merging 'parquet/schema/GroupType.class' with strategy 'first'
[warn] Merging 'parquet/schema/IncompatibleSchemaModificationException.class' with strategy 'first'
[warn] Merging 'parquet/schema/MessageType.class' with strategy 'first'
[warn] Merging 'parquet/schema/MessageTypeParser$Tokenizer.class' with strategy 'first'
[warn] Merging 'parquet/schema/MessageTypeParser.class' with strategy 'first'
[warn] Merging 'parquet/schema/OriginalType.class' with strategy 'first'
[warn] Merging 'parquet/schema/PrimitiveType$1.class' with strategy 'first'
[warn] Merging 'parquet/schema/PrimitiveType$PrimitiveTypeName$1.class' with strategy 'first'
[warn] Merging 'parquet/schema/PrimitiveType$PrimitiveTypeName$2.class' with strategy 'first'
[warn] Merging 'parquet/schema/PrimitiveType$PrimitiveTypeName$3.class' with strategy 'first'
[warn] Merging 'parquet/schema/PrimitiveType$PrimitiveTypeName$4.class' with strategy 'first'
[warn] Merging 'parquet/schema/PrimitiveType$PrimitiveTypeName$5.class' with strategy 'first'
[warn] Merging 'parquet/schema/PrimitiveType$PrimitiveTypeName$6.class' with strategy 'first'
[warn] Merging 'parquet/schema/PrimitiveType$PrimitiveTypeName$7.class' with strategy 'first'
[warn] Merging 'parquet/schema/PrimitiveType$PrimitiveTypeName$8.class' with strategy 'first'
[warn] Merging 'parquet/schema/PrimitiveType$PrimitiveTypeName.class' with strategy 'first'
[warn] Merging 'parquet/schema/PrimitiveType$PrimitiveTypeNameConverter.class' with strategy 'first'
[warn] Merging 'parquet/schema/PrimitiveType.class' with strategy 'first'
[warn] Merging 'parquet/schema/Type$1.class' with strategy 'first'
[warn] Merging 'parquet/schema/Type$Repetition$1.class' with strategy 'first'
[warn] Merging 'parquet/schema/Type$Repetition$2.class' with strategy 'first'
[warn] Merging 'parquet/schema/Type$Repetition$3.class' with strategy 'first'
[warn] Merging 'parquet/schema/Type$Repetition.class' with strategy 'first'
[warn] Merging 'parquet/schema/Type.class' with strategy 'first'
[warn] Merging 'parquet/schema/TypeConverter.class' with strategy 'first'
[warn] Merging 'parquet/schema/TypeVisitor.class' with strategy 'first'
[warn] Merging 'plugin.properties' with strategy 'first'
[warn] Merging 'plugin.xml' with strategy 'first'
[warn] Merging 'reference.conf' with strategy 'concat'
[warn] Merging 'rootdoc.txt' with strategy 'first'
[warn] Strategy 'concat' was applied to a file
[warn] Strategy 'discard' was applied to 1756 files
[warn] Strategy 'filterDistinctLines' was applied to 7 files
[warn] Strategy 'first' was applied to 2184 files
[info] SHA-1: 49a71f3ae33f0ff59e39f94887d8c5315187c6fd
[info] Packaging /usr/local/spark-1.2.1/examples/target/scala-2.10/spark-examples-1.2.1-hadoop2.6.0.jar ...
[info] Done packaging.
[info] Done packaging.
[success] Total time: 3270 s, completed Jun 26, 2015 12:58:48 AM
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/pyspark
Python 2.7.3 (default, Dec 18 2014, 19:10:20) 
[GCC 4.6.3] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Found multiple Spark assembly jars in /usr/local/spark-1.2.1/assembly/target/scala-2.10:
/usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar
/usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.6.0.jar
Please remove all but one jar.
Traceback (most recent call last):
  File "/usr/local/spark-1.2.1/python/pyspark/shell.py", line 45, in <module>
    sc = SparkContext(appName="PySparkShell", pyFiles=add_files)
  File "/usr/local/spark-1.2.1/python/pyspark/context.py", line 102, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway)
  File "/usr/local/spark-1.2.1/python/pyspark/context.py", line 212, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway()
  File "/usr/local/spark-1.2.1/python/pyspark/java_gateway.py", line 73, in launch_gateway
    raise Exception(error_msg)
Exception: Launching GatewayServer failed with exit code 1!
Warning: Expected GatewayServer to output a port, but found the following:

--------------------------------------------------------------

--------------------------------------------------------------

>>> exit
Use exit() or Ctrl-D (i.e. EOF) to exit
>>> 
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ mv ./assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop
spark-assembly-1.2.1-hadoop1.0.4.jar  spark-assembly-1.2.1-hadoop2.6.0.jar  
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ mv ./assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar ~/Runnable/spark-assembly-1.2.1-hadoop1.0.4.jar
mv: cannot move `./assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar' to `/home/hduser/Runnable/spark-assembly-1.2.1-hadoop1.0.4.jar': No such file or directory
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ mv ./assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar ~/Runnable/
mv: cannot move `./assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar' to `/home/hduser/Runnable/': Not a directory
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ sudo mv ./assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar ~/Runnable/spark-assembly-1.2.1-hadoop1.0.4.jar
[sudo] password for hduser: 
mv: cannot move `./assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar' to `/home/hduser/Runnable/spark-assembly-1.2.1-hadoop1.0.4.jar': No such file or directory
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ exit
logout
arinjoy@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ su - hduser
Password: 
su: Authentication failure
arinjoy@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ su - hduser
Password: 
hduser@arinjoy-Inspiron-3521:~$ cd /usr/local/spark-1.2.1/bin/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ./spark-submit ~/sampleHiveSpark.py 
Found multiple Spark assembly jars in /usr/local/spark-1.2.1/assembly/target/scala-2.10:
/usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar
/usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.6.0.jar
Please remove all but one jar.

hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ mv /usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar
mv: missing destination file operand after `/usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar'
Try `mv --help' for more information.
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ mv /usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar ~/Desktop
mv: cannot move `/usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar' to `/home/hduser/Desktop/spark-assembly-1.2.1-hadoop1.0.4.jar': Permission denied
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ sudo mv /usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop1.0.4.jar ~/Desktop/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ./spark-submit ~/sampleHiveSpark.py Spark assembly has been built with Hive, including Datanucleus jars on classpath
Traceback (most recent call last):
  File "/home/hduser/sampleHiveSpark.py", line 9, in <module>
    sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
  File "/usr/local/spark-1.2.1/python/pyspark/sql.py", line 1620, in sql
    return SchemaRDD(self._ssql_ctx.sql(sqlQuery).toJavaSchemaRDD(), self)
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o19.sql.
: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:235)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:231)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.sql.hive.HiveContext.x$3$lzycompute(HiveContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.x$3(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf$lzycompute(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.<init>(HiveMetastoreCatalog.scala:55)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext$$anon$4.<init>(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:262)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:411)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)
	... 30 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)
	... 35 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)
	... 40 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	... 69 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85)
	... 87 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238)
	... 89 more

hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ hive --service metastore &
[1] 26061
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory
Starting Hive Metastore Server
org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:9083.
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:109)
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:91)
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:83)
	at org.apache.hadoop.hive.metastore.TServerSocketKeepAlive.<init>(TServerSocketKeepAlive.java:34)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:5936)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:5877)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Exception in thread "main" org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:9083.
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:109)
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:91)
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:83)
	at org.apache.hadoop.hive.metastore.TServerSocketKeepAlive.<init>(TServerSocketKeepAlive.java:34)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:5936)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:5877)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
CCCCZZ^C
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ cd ../conf/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ ls
fairscheduler.xml.template  log4j.properties           metrics.properties.template  spark-defaults.conf.template
hive-site.xml               log4j.properties.template  slaves.template              spark-env.sh.template
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ vim hive-site.xml 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ cd ../../hive/conf/
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ ls
beeline-log4j.properties  hive-default.xml  hive-exec-log4j.properties  hive-site.xml    metastore_db
derby.log                 hive-env.sh       hive-log4j.properties       ivysettings.xml
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ vim hive-default.xml 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ vim hive-site.xml 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ hive
ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory
[Fatal Error] hive-site.xml:32:5: The element type "value" must be terminated by the matching end-tag "</value>".
15/06/26 01:11:02 FATAL conf.Configuration: error parsing conf file:/usr/local/hive/conf/hive-site.xml
org.xml.sax.SAXParseException; systemId: file:/usr/local/hive/conf/hive-site.xml; lineNumber: 32; columnNumber: 5; The element type "value" must be terminated by the matching end-tag "</value>".
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2352)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2340)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2408)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2374)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2281)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1108)
	at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:2605)
	at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:2626)
	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:2696)
	at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:2641)
	at org.apache.hadoop.hive.common.LogUtils.initHiveLog4jCommon(LogUtils.java:74)
	at org.apache.hadoop.hive.common.LogUtils.initHiveLog4j(LogUtils.java:58)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:637)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Exception in thread "main" java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/usr/local/hive/conf/hive-site.xml; lineNumber: 32; columnNumber: 5; The element type "value" must be terminated by the matching end-tag "</value>".
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2517)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2374)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2281)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1108)
	at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:2605)
	at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:2626)
	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:2696)
	at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:2641)
	at org.apache.hadoop.hive.common.LogUtils.initHiveLog4jCommon(LogUtils.java:74)
	at org.apache.hadoop.hive.common.LogUtils.initHiveLog4j(LogUtils.java:58)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:637)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: org.xml.sax.SAXParseException; systemId: file:/usr/local/hive/conf/hive-site.xml; lineNumber: 32; columnNumber: 5; The element type "value" must be terminated by the matching end-tag "</value>".
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2352)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2340)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2408)
	... 17 more
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ vim hive-site.xml 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ vim hive-site.xml 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ vim hive-default.xml 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ hive
ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory

Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j.properties
hive> show tables;
OK
saurzcode
Time taken: 4.771 seconds, Fetched: 1 row(s)
hive> CREATE TABLE user2(id INT, name STRING, City STRING, State STRING, Country STRING) 
    >  ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
    >  LINES TERMINATED BY '\n' STORED AS TEXTFILE;
OK
Time taken: 7.975 seconds
hive> show tables;
OK
saurzcode
user2
Time taken: 0.069 seconds, Fetched: 2 row(s)
hive> CREATE TABLE user3(id INT, name STRING, City STRING, State STRING, Country STRING) 
    >  ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
    >  LINES TERMINATED BY '\n' STORED AS TEXTFILE;
OK
Time taken: 1.089 seconds
hive> show tables;
OK
saurzcode
user2
user3
Time taken: 0.075 seconds, Fetched: 3 row(s)
hive> exit;
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ vim hive-site.xml 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ hive
ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory

Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j.properties
hive> show tables;
OK
saurzcode
user2
Time taken: 1.647 seconds, Fetched: 2 row(s)
hive> drop table user2;
OK
Time taken: 2.616 seconds
hive> CREATE TABLE user3(id INT, name STRING, City STRING, State STRING, Country STRING) 
    >  ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
    >  LINES TERMINATED BY '\n' STORED AS TEXTFILE;
OK
Time taken: 1.882 seconds
hive> exit
    > ;
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ ./spark-submit ~/sampleHiveSpark.py 
-su: ./spark-submit: No such file or directory
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ cd ..
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ ./bin/spark-submit ~/sampleHiveSpark.py 
-su: ./bin/spark-submit: No such file or directory
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ cd ../spark-1.2.1/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/spark-submit ~/sampleHiveSpark.py 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Traceback (most recent call last):
  File "/home/hduser/sampleHiveSpark.py", line 9, in <module>
    sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
  File "/usr/local/spark-1.2.1/python/pyspark/sql.py", line 1620, in sql
    return SchemaRDD(self._ssql_ctx.sql(sqlQuery).toJavaSchemaRDD(), self)
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o19.sql.
: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:235)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:231)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.sql.hive.HiveContext.x$3$lzycompute(HiveContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.x$3(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf$lzycompute(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.<init>(HiveMetastoreCatalog.scala:55)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext$$anon$4.<init>(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:262)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:411)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)
	... 30 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)
	... 35 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)
	... 40 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	... 69 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85)
	... 87 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238)
	... 89 more

hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/spark-submit ~/sampleHiveSpark.py 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Traceback (most recent call last):
  File "/home/hduser/sampleHiveSpark.py", line 9, in <module>
    sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
  File "/usr/local/spark-1.2.1/python/pyspark/sql.py", line 1620, in sql
    return SchemaRDD(self._ssql_ctx.sql(sqlQuery).toJavaSchemaRDD(), self)
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o19.sql.
: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:235)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:231)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.sql.hive.HiveContext.x$3$lzycompute(HiveContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.x$3(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf$lzycompute(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.<init>(HiveMetastoreCatalog.scala:55)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext$$anon$4.<init>(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:262)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:411)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)
	... 30 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)
	... 35 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)
	... 40 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	... 69 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85)
	... 87 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238)
	... 89 more

[1]+  Killed                  hive --service metastore  (wd: /usr/local/spark-1.2.1/bin)
(wd now: /usr/local/spark-1.2.1)
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ cd conf/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ ls
fairscheduler.xml.template  log4j.properties           metrics.properties.template  spark-defaults.conf.template
hive-site.xml               log4j.properties.template  slaves.template              spark-env.sh.template
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ cd ../../hive/conf/
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ ls
beeline-log4j.properties  hive-default.xml  hive-exec-log4j.properties  hive-site.xml    metastore_db
derby.log                 hive-env.sh       hive-log4j.properties       ivysettings.xml
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ cd metastore_db/
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf/metastore_db$ ls
dbex.lck  db.lck  log  README_DO_NOT_TOUCH_FILES.txt  seg0  service.properties  tmp
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf/metastore_db$ rm *.lck
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf/metastore_db$ cd ../../
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ ./bin/hive --service metastore &
[1] 27507
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory
Starting Hive Metastore Server
org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:9083.
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:109)
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:91)
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:83)
	at org.apache.hadoop.hive.metastore.TServerSocketKeepAlive.<init>(TServerSocketKeepAlive.java:34)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:5936)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:5877)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Exception in thread "main" org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:9083.
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:109)
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:91)
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:83)
	at org.apache.hadoop.hive.metastore.TServerSocketKeepAlive.<init>(TServerSocketKeepAlive.java:34)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:5936)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:5877)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
^C
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ ./bin/hive --service metastore &
[2] 27706
[1]   Killed                  ./bin/hive --service metastore
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory
Starting Hive Metastore Server
^C
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ hive -hiveconf hive.root.logger=DEBUG,console
ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory
15/06/26 01:38:35 [main]: WARN common.LogUtils: DEPRECATED: Ignoring hive-default.xml found on the CLASSPATH at /usr/local/hive/conf/hive-default.xml
15/06/26 01:38:35 [main]: DEBUG common.LogUtils: Using hive-site.xml found on CLASSPATH at /usr/local/hive/conf/hive-site.xml

Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j.properties
15/06/26 01:38:36 [main]: INFO SessionState: 
Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j.properties
15/06/26 01:38:36 [main]: DEBUG parse.VariableSubstitution: Substitution is on: hive
15/06/26 01:38:36 [main]: DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[Rate of successful kerberos logins and latency (milliseconds)], about=, valueName=Time, type=DEFAULT, always=false, sampleName=Ops)
15/06/26 01:38:36 [main]: DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[Rate of failed kerberos logins and latency (milliseconds)], about=, valueName=Time, type=DEFAULT, always=false, sampleName=Ops)
15/06/26 01:38:36 [main]: DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[GetGroups], about=, valueName=Time, type=DEFAULT, always=false, sampleName=Ops)
15/06/26 01:38:36 [main]: DEBUG impl.MetricsSystemImpl: UgiMetrics, User and group related metrics
15/06/26 01:38:36 [main]: DEBUG util.KerberosName: Kerberos krb5 configuration not found, setting default realm to empty
15/06/26 01:38:36 [main]: DEBUG security.Groups:  Creating new Groups object
15/06/26 01:38:36 [main]: DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...
15/06/26 01:38:36 [main]: DEBUG util.NativeCodeLoader: Loaded the native-hadoop library
15/06/26 01:38:36 [main]: DEBUG security.JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution
15/06/26 01:38:36 [main]: DEBUG security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping
15/06/26 01:38:36 [main]: DEBUG security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
15/06/26 01:38:36 [main]: DEBUG security.UserGroupInformation: hadoop login
15/06/26 01:38:36 [main]: DEBUG security.UserGroupInformation: hadoop login commit
15/06/26 01:38:36 [main]: DEBUG security.UserGroupInformation: using local user:UnixPrincipal: hduser
15/06/26 01:38:36 [main]: DEBUG security.UserGroupInformation: Using user: "UnixPrincipal: hduser" with name hduser
15/06/26 01:38:36 [main]: DEBUG security.UserGroupInformation: User entry: "hduser"
15/06/26 01:38:36 [main]: DEBUG security.UserGroupInformation: UGI loginUser:hduser (auth:SIMPLE)
15/06/26 01:38:36 [main]: INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083
15/06/26 01:38:36 [main]: WARN hive.metastore: Failed to connect to the MetaStore Server...
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 25 more
15/06/26 01:38:36 [main]: INFO hive.metastore: Waiting 1 seconds before next connection attempt.
15/06/26 01:38:37 [main]: INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083
15/06/26 01:38:37 [main]: WARN hive.metastore: Failed to connect to the MetaStore Server...
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 25 more
15/06/26 01:38:37 [main]: INFO hive.metastore: Waiting 1 seconds before next connection attempt.
15/06/26 01:38:38 [main]: INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083
15/06/26 01:38:38 [main]: WARN hive.metastore: Failed to connect to the MetaStore Server...
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 25 more
15/06/26 01:38:38 [main]: INFO hive.metastore: Waiting 1 seconds before next connection attempt.
15/06/26 01:38:39 [main]: WARN metadata.Hive: Failed to access metastore. This class should not accessed in runtime.
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1236)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	... 11 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	... 17 more
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 25 more
)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:466)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	... 22 more
15/06/26 01:38:39 [main]: INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083
15/06/26 01:38:39 [main]: WARN hive.metastore: Failed to connect to the MetaStore Server...
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 22 more
15/06/26 01:38:39 [main]: INFO hive.metastore: Waiting 1 seconds before next connection attempt.
15/06/26 01:38:40 [main]: INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083
15/06/26 01:38:40 [main]: WARN hive.metastore: Failed to connect to the MetaStore Server...
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 22 more
15/06/26 01:38:40 [main]: INFO hive.metastore: Waiting 1 seconds before next connection attempt.
15/06/26 01:38:41 [main]: INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083
15/06/26 01:38:41 [main]: WARN hive.metastore: Failed to connect to the MetaStore Server...
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 22 more
15/06/26 01:38:41 [main]: INFO hive.metastore: Waiting 1 seconds before next connection attempt.
Exception in thread "main" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:519)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	... 8 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	... 14 more
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 22 more
)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:466)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	... 19 more
[2]+  Killed                  ./bin/hive --service metastore
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ 
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ hive -hiveconf hive.root.logger=DEBUG,console
ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory
15/06/26 01:39:09 [main]: WARN common.LogUtils: DEPRECATED: Ignoring hive-default.xml found on the CLASSPATH at /usr/local/hive/conf/hive-default.xml
15/06/26 01:39:09 [main]: DEBUG common.LogUtils: Using hive-site.xml found on CLASSPATH at /usr/local/hive/conf/hive-site.xml

Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j.properties
15/06/26 01:39:09 [main]: INFO SessionState: 
Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j.properties
15/06/26 01:39:09 [main]: DEBUG parse.VariableSubstitution: Substitution is on: hive
15/06/26 01:39:09 [main]: DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[Rate of successful kerberos logins and latency (milliseconds)], about=, valueName=Time, type=DEFAULT, always=false, sampleName=Ops)
15/06/26 01:39:09 [main]: DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[Rate of failed kerberos logins and latency (milliseconds)], about=, valueName=Time, type=DEFAULT, always=false, sampleName=Ops)
15/06/26 01:39:09 [main]: DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[GetGroups], about=, valueName=Time, type=DEFAULT, always=false, sampleName=Ops)
15/06/26 01:39:09 [main]: DEBUG impl.MetricsSystemImpl: UgiMetrics, User and group related metrics
15/06/26 01:39:10 [main]: DEBUG util.KerberosName: Kerberos krb5 configuration not found, setting default realm to empty
15/06/26 01:39:10 [main]: DEBUG security.Groups:  Creating new Groups object
15/06/26 01:39:10 [main]: DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...
15/06/26 01:39:10 [main]: DEBUG util.NativeCodeLoader: Loaded the native-hadoop library
15/06/26 01:39:10 [main]: DEBUG security.JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution
15/06/26 01:39:10 [main]: DEBUG security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping
15/06/26 01:39:10 [main]: DEBUG security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
15/06/26 01:39:10 [main]: DEBUG security.UserGroupInformation: hadoop login
15/06/26 01:39:10 [main]: DEBUG security.UserGroupInformation: hadoop login commit
15/06/26 01:39:10 [main]: DEBUG security.UserGroupInformation: using local user:UnixPrincipal: hduser
15/06/26 01:39:10 [main]: DEBUG security.UserGroupInformation: Using user: "UnixPrincipal: hduser" with name hduser
15/06/26 01:39:10 [main]: DEBUG security.UserGroupInformation: User entry: "hduser"
15/06/26 01:39:10 [main]: DEBUG security.UserGroupInformation: UGI loginUser:hduser (auth:SIMPLE)
15/06/26 01:39:10 [main]: INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083
15/06/26 01:39:10 [main]: WARN hive.metastore: Failed to connect to the MetaStore Server...
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 25 more
15/06/26 01:39:10 [main]: INFO hive.metastore: Waiting 1 seconds before next connection attempt.
15/06/26 01:39:11 [main]: INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083
15/06/26 01:39:11 [main]: WARN hive.metastore: Failed to connect to the MetaStore Server...
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 25 more
15/06/26 01:39:11 [main]: INFO hive.metastore: Waiting 1 seconds before next connection attempt.
15/06/26 01:39:12 [main]: INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083
15/06/26 01:39:12 [main]: WARN hive.metastore: Failed to connect to the MetaStore Server...
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 25 more
15/06/26 01:39:12 [main]: INFO hive.metastore: Waiting 1 seconds before next connection attempt.
15/06/26 01:39:13 [main]: WARN metadata.Hive: Failed to access metastore. This class should not accessed in runtime.
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1236)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	... 11 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	... 17 more
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 25 more
)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:466)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	... 22 more
15/06/26 01:39:13 [main]: INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083
15/06/26 01:39:13 [main]: WARN hive.metastore: Failed to connect to the MetaStore Server...
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 22 more
15/06/26 01:39:13 [main]: INFO hive.metastore: Waiting 1 seconds before next connection attempt.
15/06/26 01:39:14 [main]: INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083
15/06/26 01:39:14 [main]: WARN hive.metastore: Failed to connect to the MetaStore Server...
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 22 more
15/06/26 01:39:14 [main]: INFO hive.metastore: Waiting 1 seconds before next connection attempt.
15/06/26 01:39:15 [main]: INFO hive.metastore: Trying to connect to metastore with URI thrift://127.0.0.1:9083
15/06/26 01:39:15 [main]: WARN hive.metastore: Failed to connect to the MetaStore Server...
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 22 more
15/06/26 01:39:15 [main]: INFO hive.metastore: Waiting 1 seconds before next connection attempt.
Exception in thread "main" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:519)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	... 8 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	... 14 more
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 22 more
)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:466)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	... 19 more
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ hive
ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory

Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j.properties
Exception in thread "main" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:519)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	... 8 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	... 14 more
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	... 22 more
)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:466)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	... 19 more
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ cd /conf/
-su: cd: /conf/: No such file or directory
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ cd conf/
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ ls
beeline-log4j.properties  hive-default.xml  hive-exec-log4j.properties  hive-site.xml    metastore_db
derby.log                 hive-env.sh       hive-log4j.properties       ivysettings.xml
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ vim hive-site.xml 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ vim hive-default.xml 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ hive
ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory

Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j.properties
hive> show tables;
OK
saurzcode
user3
Time taken: 1.609 seconds, Fetched: 2 row(s)
hive> exit;
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ hive -hiveconf hive.root.logger=DEBUG,console
ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory
15/06/26 01:42:21 [main]: WARN common.LogUtils: DEPRECATED: Ignoring hive-default.xml found on the CLASSPATH at /usr/local/hive/conf/hive-default.xml
15/06/26 01:42:21 [main]: DEBUG common.LogUtils: Using hive-site.xml found on CLASSPATH at /usr/local/hive/conf/hive-site.xml

Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j.properties
15/06/26 01:42:21 [main]: INFO SessionState: 
Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j.properties
15/06/26 01:42:21 [main]: DEBUG parse.VariableSubstitution: Substitution is on: hive
15/06/26 01:42:21 [main]: DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)], about=, always=false, type=DEFAULT, sampleName=Ops)
15/06/26 01:42:21 [main]: DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)], about=, always=false, type=DEFAULT, sampleName=Ops)
15/06/26 01:42:21 [main]: DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[GetGroups], about=, always=false, type=DEFAULT, sampleName=Ops)
15/06/26 01:42:21 [main]: DEBUG impl.MetricsSystemImpl: UgiMetrics, User and group related metrics
15/06/26 01:42:22 [main]: DEBUG util.KerberosName: Kerberos krb5 configuration not found, setting default realm to empty
15/06/26 01:42:22 [main]: DEBUG security.Groups:  Creating new Groups object
15/06/26 01:42:22 [main]: DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...
15/06/26 01:42:22 [main]: DEBUG util.NativeCodeLoader: Loaded the native-hadoop library
15/06/26 01:42:22 [main]: DEBUG security.JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution
15/06/26 01:42:22 [main]: DEBUG security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping
15/06/26 01:42:22 [main]: DEBUG security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
15/06/26 01:42:22 [main]: DEBUG security.UserGroupInformation: hadoop login
15/06/26 01:42:22 [main]: DEBUG security.UserGroupInformation: hadoop login commit
15/06/26 01:42:22 [main]: DEBUG security.UserGroupInformation: using local user:UnixPrincipal: hduser
15/06/26 01:42:22 [main]: DEBUG security.UserGroupInformation: Using user: "UnixPrincipal: hduser" with name hduser
15/06/26 01:42:22 [main]: DEBUG security.UserGroupInformation: User entry: "hduser"
15/06/26 01:42:22 [main]: DEBUG security.UserGroupInformation: UGI loginUser:hduser (auth:SIMPLE)
15/06/26 01:42:22 [main]: INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.validateConstraints value null from  jpox.properties with false
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.cache.level2 value null from  jpox.properties with false
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding javax.jdo.option.ConnectionUserName value null from  jpox.properties with hiveuser
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.connectionPoolingType value null from  jpox.properties with BONECP
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.fixedDatastore value null from  jpox.properties with true
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding javax.jdo.option.ConnectionDriverName value null from  jpox.properties with com.mysql.jdbc.Driver
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.validateColumns value null from  jpox.properties with false
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.validateTables value null from  jpox.properties with false
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.cache.level2.type value null from  jpox.properties with none
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.plugin.pluginRegistryBundleCheck value null from  jpox.properties with LOG
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.autoStartMechanismMode value null from  jpox.properties with checked
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.autoCreateSchema value null from  jpox.properties with false
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding javax.jdo.option.NonTransactionalRead value null from  jpox.properties with true
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.rdbms.useLegacyNativeValueStrategy value null from  jpox.properties with true
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.identifierFactory value null from  jpox.properties with datanucleus1
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding javax.jdo.option.ConnectionURL value null from  jpox.properties with jdbc:mysql://localhost/metastore
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding javax.jdo.option.DetachAllOnCommit value null from  jpox.properties with true
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding hive.metastore.integral.jdo.pushdown value null from  jpox.properties with false
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.storeManagerType value null from  jpox.properties with rdbms
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.transactionIsolation value null from  jpox.properties with read-committed
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding javax.jdo.PersistenceManagerFactoryClass value null from  jpox.properties with org.datanucleus.api.jdo.JDOPersistenceManagerFactory
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: Overriding javax.jdo.option.Multithreaded value null from  jpox.properties with true
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.rdbms.useLegacyNativeValueStrategy = true
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: hive.metastore.integral.jdo.pushdown = false
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.autoStartMechanismMode = checked
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: javax.jdo.option.Multithreaded = true
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.identifierFactory = datanucleus1
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.transactionIsolation = read-committed
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.validateTables = false
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: javax.jdo.option.ConnectionURL = jdbc:mysql://localhost/metastore
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: javax.jdo.option.DetachAllOnCommit = true
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: javax.jdo.option.NonTransactionalRead = true
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.fixedDatastore = true
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.validateConstraints = false
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: javax.jdo.option.ConnectionDriverName = com.mysql.jdbc.Driver
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: javax.jdo.option.ConnectionUserName = hiveuser
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.validateColumns = false
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.cache.level2 = false
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.plugin.pluginRegistryBundleCheck = LOG
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.cache.level2.type = none
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: javax.jdo.PersistenceManagerFactoryClass = org.datanucleus.api.jdo.JDOPersistenceManagerFactory
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.autoCreateSchema = false
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.storeManagerType = rdbms
15/06/26 01:42:22 [main]: DEBUG metastore.ObjectStore: datanucleus.connectionPoolingType = BONECP
15/06/26 01:42:22 [main]: INFO metastore.ObjectStore: ObjectStore, initialize called
15/06/26 01:42:22 [main]: DEBUG bonecp.BoneCPDataSource: JDBC URL = jdbc:mysql://localhost/metastore, Username = hiveuser, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
15/06/26 01:42:23 [main]: INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
15/06/26 01:42:23 [main]: DEBUG bonecp.BoneCPDataSource: JDBC URL = jdbc:mysql://localhost/metastore, Username = hiveuser, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
15/06/26 01:42:23 [main]: DEBUG metastore.MetaStoreDirectSql: Direct SQL query in 7.072542ms + 0.125941ms, the query is [SET @@session.sql_mode=ANSI_QUOTES]
15/06/26 01:42:25 [main]: INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL
15/06/26 01:42:25 [main]: DEBUG metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5d4ce711, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2eaa4bf8 created in the thread with id: 1
15/06/26 01:42:25 [main]: INFO metastore.ObjectStore: Initialized ObjectStore
15/06/26 01:42:25 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6717)
15/06/26 01:42:25 [main]: DEBUG metastore.ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6731)
15/06/26 01:42:25 [main]: DEBUG metastore.ObjectStore: Found expected HMS version of 1.2.0
15/06/26 01:42:25 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.start(ObjectStore.java:2408)
15/06/26 01:42:25 [main]: DEBUG metastore.MetaStoreDirectSql: Direct SQL query in 0.313415ms + 0.025522ms, the query is [SET @@session.sql_mode=ANSI_QUOTES]
15/06/26 01:42:25 [main]: DEBUG metastore.MetaStoreDirectSql: getDatabase: directsql returning db default locn[hdfs://localhost:54310/user/hive/warehouse] desc [Default Hive database] owner [public] ownertype [ROLE]
15/06/26 01:42:25 [main]: DEBUG metastore.ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2445)
15/06/26 01:42:25 [main]: DEBUG metastore.ObjectStore: db details for db default retrieved using SQL in 22.902009ms
15/06/26 01:42:25 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3220)
15/06/26 01:42:25 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3527)
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3533)
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Rollback transaction, isActive: true at:
	org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3233)
15/06/26 01:42:26 [main]: DEBUG metastore.HiveMetaStore: admin role already exists
InvalidObjectException(message:Role admin already exists.)
	at org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3223)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
	at com.sun.proxy.$Proxy5.addRole(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultRoles_core(HiveMetaStore.java:654)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultRoles(HiveMetaStore.java:643)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:460)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5730)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
15/06/26 01:42:26 [main]: INFO metastore.HiveMetaStore: Added admin role in metastore
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3220)
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3527)
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3533)
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Rollback transaction, isActive: true at:
	org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3233)
15/06/26 01:42:26 [main]: DEBUG metastore.HiveMetaStore: public role already exists
InvalidObjectException(message:Role public already exists.)
	at org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3223)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
	at com.sun.proxy.$Proxy5.addRole(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultRoles_core(HiveMetaStore.java:663)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultRoles(HiveMetaStore.java:643)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:460)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5730)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
15/06/26 01:42:26 [main]: INFO metastore.HiveMetaStore: Added public role in metastore
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.grantPrivileges(ObjectStore.java:3912)
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3527)
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3533)
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalGlobalGrants(ObjectStore.java:4397)
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalGlobalGrants(ObjectStore.java:4407)
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Rollback transaction, isActive: true at:
	org.apache.hadoop.hive.metastore.ObjectStore.grantPrivileges(ObjectStore.java:4115)
15/06/26 01:42:26 [main]: DEBUG metastore.HiveMetaStore: Failed while granting global privs to admin
InvalidObjectException(message:All is already granted by admin)
	at org.apache.hadoop.hive.metastore.ObjectStore.grantPrivileges(ObjectStore.java:3948)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
	at com.sun.proxy.$Proxy5.grantPrivileges(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultRoles_core(HiveMetaStore.java:677)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultRoles(HiveMetaStore.java:643)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:460)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5730)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3000)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:500)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
15/06/26 01:42:26 [main]: INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
15/06/26 01:42:26 [main]: INFO metastore.HiveMetaStore: 0: get_all_databases
15/06/26 01:42:26 [main]: INFO HiveMetaStore.audit: ugi=hduser	ip=unknown-ip-addr	cmd=get_all_databases	
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getDatabases(ObjectStore.java:677)
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getDatabases(ObjectStore.java:701)
15/06/26 01:42:26 [main]: INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
15/06/26 01:42:26 [main]: INFO HiveMetaStore.audit: ugi=hduser	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getFunctions(ObjectStore.java:7013)
15/06/26 01:42:26 [main]: DEBUG metastore.ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getFunctions(ObjectStore.java:7041)
15/06/26 01:42:26 [main]: DEBUG hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
15/06/26 01:42:26 [main]: DEBUG hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = false
15/06/26 01:42:26 [main]: DEBUG hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
15/06/26 01:42:26 [main]: DEBUG hdfs.BlockReaderLocal: dfs.domain.socket.path = 
15/06/26 01:42:26 [main]: DEBUG hdfs.DFSClient: No KeyProvider found.
15/06/26 01:42:26 [main]: DEBUG retry.RetryUtils: multipleLinearRandomRetry = null
15/06/26 01:42:26 [main]: DEBUG ipc.Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@6abd7d51
15/06/26 01:42:26 [main]: DEBUG ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7e5d4811
15/06/26 01:42:27 [Thread-5]: DEBUG unix.DomainSocketWatcher: org.apache.hadoop.net.unix.DomainSocketWatcher$1@6ef7be6a: starting with interruptCheckPeriodMs = 60000
15/06/26 01:42:27 [main]: DEBUG util.PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.
15/06/26 01:42:27 [main]: DEBUG sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
15/06/26 01:42:27 [main]: DEBUG ipc.Client: The ping interval is 60000 ms.
15/06/26 01:42:27 [main]: DEBUG ipc.Client: Connecting to localhost/127.0.0.1:54310
15/06/26 01:42:27 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser: starting, having connections 1
15/06/26 01:42:27 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser sending #0
15/06/26 01:42:27 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser got value #0
15/06/26 01:42:27 [main]: DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 84ms
15/06/26 01:42:27 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser sending #1
15/06/26 01:42:27 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser got value #1
15/06/26 01:42:27 [main]: DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 4ms
15/06/26 01:42:27 [main]: DEBUG session.SessionState: HDFS root scratch dir: /tmp/hive with schema null, permission: rwx-wx-wx
15/06/26 01:42:27 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser sending #2
15/06/26 01:42:27 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser got value #2
15/06/26 01:42:27 [main]: DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms
15/06/26 01:42:27 [main]: DEBUG nativeio.NativeIO: Initialized cache for IDs to User/Group mapping with a  cache timeout of 14400 seconds.
15/06/26 01:42:27 [main]: INFO session.SessionState: Created local directory: /tmp/2475f092-d0c3-43ca-95b9-b1a3a68a1abc_resources
15/06/26 01:42:27 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser sending #3
15/06/26 01:42:27 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser got value #3
15/06/26 01:42:27 [main]: DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 3ms
15/06/26 01:42:27 [main]: DEBUG hdfs.DFSClient: /tmp/hive/hduser/2475f092-d0c3-43ca-95b9-b1a3a68a1abc: masked=rwx------
15/06/26 01:42:27 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser sending #4
15/06/26 01:42:27 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser got value #4
15/06/26 01:42:27 [main]: DEBUG ipc.ProtobufRpcEngine: Call: mkdirs took 23ms
15/06/26 01:42:27 [main]: INFO session.SessionState: Created HDFS directory: /tmp/hive/hduser/2475f092-d0c3-43ca-95b9-b1a3a68a1abc
15/06/26 01:42:27 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser sending #5
15/06/26 01:42:27 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser got value #5
15/06/26 01:42:27 [main]: DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 3ms
15/06/26 01:42:27 [main]: INFO session.SessionState: Created local directory: /tmp/hduser/2475f092-d0c3-43ca-95b9-b1a3a68a1abc
15/06/26 01:42:27 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser sending #6
15/06/26 01:42:27 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser got value #6
15/06/26 01:42:27 [main]: DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 2ms
15/06/26 01:42:27 [main]: DEBUG hdfs.DFSClient: /tmp/hive/hduser/2475f092-d0c3-43ca-95b9-b1a3a68a1abc/_tmp_space.db: masked=rwx------
15/06/26 01:42:27 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser sending #7
15/06/26 01:42:27 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser got value #7
15/06/26 01:42:27 [main]: DEBUG ipc.ProtobufRpcEngine: Call: mkdirs took 8ms
15/06/26 01:42:27 [main]: INFO session.SessionState: Created HDFS directory: /tmp/hive/hduser/2475f092-d0c3-43ca-95b9-b1a3a68a1abc/_tmp_space.db
15/06/26 01:42:27 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser sending #8
15/06/26 01:42:27 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser got value #8
15/06/26 01:42:27 [main]: DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 2ms
15/06/26 01:42:27 [main]: DEBUG CliDriver: CliDriver inited with classpath /usr/local/hive/conf:/usr/local/hive/lib/accumulo-core-1.6.0.jar:/usr/local/hive/lib/accumulo-fate-1.6.0.jar:/usr/local/hive/lib/accumulo-start-1.6.0.jar:/usr/local/hive/lib/accumulo-trace-1.6.0.jar:/usr/local/hive/lib/activation-1.1.jar:/usr/local/hive/lib/ant-1.9.1.jar:/usr/local/hive/lib/ant-launcher-1.9.1.jar:/usr/local/hive/lib/antlr-2.7.7.jar:/usr/local/hive/lib/antlr-runtime-3.4.jar:/usr/local/hive/lib/apache-log4j-extras-1.2.17.jar:/usr/local/hive/lib/asm-commons-3.1.jar:/usr/local/hive/lib/asm-tree-3.1.jar:/usr/local/hive/lib/avro-1.7.5.jar:/usr/local/hive/lib/bonecp-0.8.0.RELEASE.jar:/usr/local/hive/lib/calcite-avatica-1.2.0-incubating.jar:/usr/local/hive/lib/calcite-core-1.2.0-incubating.jar:/usr/local/hive/lib/calcite-linq4j-1.2.0-incubating.jar:/usr/local/hive/lib/commons-beanutils-1.7.0.jar:/usr/local/hive/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hive/lib/commons-cli-1.2.jar:/usr/local/hive/lib/commons-codec-1.4.jar:/usr/local/hive/lib/commons-collections-3.2.1.jar:/usr/local/hive/lib/commons-compiler-2.7.6.jar:/usr/local/hive/lib/commons-compress-1.4.1.jar:/usr/local/hive/lib/commons-configuration-1.6.jar:/usr/local/hive/lib/commons-dbcp-1.4.jar:/usr/local/hive/lib/commons-digester-1.8.jar:/usr/local/hive/lib/commons-httpclient-3.0.1.jar:/usr/local/hive/lib/commons-io-2.4.jar:/usr/local/hive/lib/commons-lang-2.6.jar:/usr/local/hive/lib/commons-logging-1.1.3.jar:/usr/local/hive/lib/commons-math-2.1.jar:/usr/local/hive/lib/commons-pool-1.5.4.jar:/usr/local/hive/lib/commons-vfs2-2.0.jar:/usr/local/hive/lib/curator-client-2.6.0.jar:/usr/local/hive/lib/curator-framework-2.6.0.jar:/usr/local/hive/lib/curator-recipes-2.6.0.jar:/usr/local/hive/lib/datanucleus-api-jdo-3.2.6.jar:/usr/local/hive/lib/datanucleus-core-3.2.10.jar:/usr/local/hive/lib/datanucleus-rdbms-3.2.9.jar:/usr/local/hive/lib/derby-10.11.1.1.jar:/usr/local/hive/lib/eigenbase-properties-1.1.5.jar:/usr/local/hive/lib/geronimo-annotation_1.0_spec-1.1.1.jar:/usr/local/hive/lib/geronimo-jaspic_1.0_spec-1.0.jar:/usr/local/hive/lib/geronimo-jta_1.1_spec-1.1.1.jar:/usr/local/hive/lib/groovy-all-2.1.6.jar:/usr/local/hive/lib/guava-14.0.1.jar:/usr/local/hive/lib/hamcrest-core-1.1.jar:/usr/local/hive/lib/hive-accumulo-handler-1.2.0.jar:/usr/local/hive/lib/hive-ant-1.2.0.jar:/usr/local/hive/lib/hive-beeline-1.2.0.jar:/usr/local/hive/lib/hive-cli-1.2.0.jar:/usr/local/hive/lib/hive-common-1.2.0.jar:/usr/local/hive/lib/hive-contrib-1.2.0.jar:/usr/local/hive/lib/hive-exec-1.2.0.jar:/usr/local/hive/lib/hive-hbase-handler-1.2.0.jar:/usr/local/hive/lib/hive-hwi-1.2.0.jar:/usr/local/hive/lib/hive-jdbc-1.2.0.jar:/usr/local/hive/lib/hive-jdbc-1.2.0-standalone.jar:/usr/local/hive/lib/hive-metastore-1.2.0.jar:/usr/local/hive/lib/hive-serde-1.2.0.jar:/usr/local/hive/lib/hive-service-1.2.0.jar:/usr/local/hive/lib/hive-shims-0.20S-1.2.0.jar:/usr/local/hive/lib/hive-shims-0.23-1.2.0.jar:/usr/local/hive/lib/hive-shims-1.2.0.jar:/usr/local/hive/lib/hive-shims-common-1.2.0.jar:/usr/local/hive/lib/hive-shims-scheduler-1.2.0.jar:/usr/local/hive/lib/hive-testutils-1.2.0.jar:/usr/local/hive/lib/httpclient-4.4.jar:/usr/local/hive/lib/httpcore-4.4.jar:/usr/local/hive/lib/ivy-2.4.0.jar:/usr/local/hive/lib/janino-2.7.6.jar:/usr/local/hive/lib/jcommander-1.32.jar:/usr/local/hive/lib/jdo-api-3.0.1.jar:/usr/local/hive/lib/jetty-all-7.6.0.v20120127.jar:/usr/local/hive/lib/jetty-all-server-7.6.0.v20120127.jar:/usr/local/hive/lib/jline-2.12.jar:/usr/local/hive/lib/joda-time-2.5.jar:/usr/local/hive/lib/jpam-1.1.jar:/usr/local/hive/lib/json-20090211.jar:/usr/local/hive/lib/jsr305-3.0.0.jar:/usr/local/hive/lib/jta-1.1.jar:/usr/local/hive/lib/junit-4.11.jar:/usr/local/hive/lib/libfb303-0.9.2.jar:/usr/local/hive/lib/libmysql-java.jar:/usr/local/hive/lib/libthrift-0.9.2.jar:/usr/local/hive/lib/log4j-1.2.16.jar:/usr/local/hive/lib/mail-1.4.1.jar:/usr/local/hive/lib/maven-scm-api-1.4.jar:/usr/local/hive/lib/maven-scm-provider-svn-commons-1.4.jar:/usr/local/hive/lib/maven-scm-provider-svnexe-1.4.jar:/usr/local/hive/lib/netty-3.7.0.Final.jar:/usr/local/hive/lib/opencsv-2.3.jar:/usr/local/hive/lib/oro-2.0.8.jar:/usr/local/hive/lib/paranamer-2.3.jar:/usr/local/hive/lib/parquet-hadoop-bundle-1.6.0.jar:/usr/local/hive/lib/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar:/usr/local/hive/lib/plexus-utils-1.5.6.jar:/usr/local/hive/lib/regexp-1.3.jar:/usr/local/hive/lib/servlet-api-2.5.jar:/usr/local/hive/lib/snappy-java-1.0.5.jar:/usr/local/hive/lib/ST4-4.0.4.jar:/usr/local/hive/lib/stax-api-1.0.1.jar:/usr/local/hive/lib/stringtemplate-3.2.1.jar:/usr/local/hive/lib/super-csv-2.2.0.jar:/usr/local/hive/lib/tempus-fugit-1.1.jar:/usr/local/hive/lib/velocity-1.5.jar:/usr/local/hive/lib/xz-1.0.jar:/usr/local/hive/lib/zookeeper-3.4.6.jar:::/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar
hive> 15/06/26 01:42:37 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser: closed
15/06/26 01:42:37 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser: stopped, remaining connections 0
exit; show tables;
15/06/26 01:43:15 [main]: INFO log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:15 [main]: INFO log.PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:15 [main]: INFO log.PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:15 [main]: DEBUG parse.VariableSubstitution: Substitution is on: show tables
15/06/26 01:43:15 [main]: INFO log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:15 [main]: INFO parse.ParseDriver: Parsing command: show tables
15/06/26 01:43:17 [main]: INFO parse.ParseDriver: Parse Completed
15/06/26 01:43:17 [main]: INFO log.PerfLogger: </PERFLOG method=parse start=1435263195893 end=1435263197122 duration=1229 from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: DEBUG ql.Driver: Encoding valid txns info 9223372036854775807:
15/06/26 01:43:17 [main]: INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: INFO ql.Driver: Semantic Analysis Completed
15/06/26 01:43:17 [main]: INFO log.PerfLogger: </PERFLOG method=semanticAnalyze start=1435263197126 end=1435263197186 duration=60 from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: DEBUG lazy.LazySimpleSerDe: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[tab_name] columnTypes=[string] separator=[[B@66594fc4] nullstring=  lastColumnTakesRest=false timestampFormats=null
15/06/26 01:43:17 [main]: INFO exec.ListSinkOperator: Initializing operator OP[0]
15/06/26 01:43:17 [main]: DEBUG lazy.LazySimpleSerDe: org.apache.hadoop.hive.serde2.DelimitedJSONSerDe initialized with: columnNames=[] columnTypes=[] separator=[[B@752a9dae] nullstring=  lastColumnTakesRest=false timestampFormats=null
15/06/26 01:43:17 [main]: INFO exec.ListSinkOperator: Initialization Done 0 OP
15/06/26 01:43:17 [main]: INFO exec.ListSinkOperator: Operator 0 OP initialized
15/06/26 01:43:17 [main]: DEBUG lazy.LazySimpleSerDe: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[tab_name] columnTypes=[string] separator=[[B@186d8cc8] nullstring=  lastColumnTakesRest=false timestampFormats=null
15/06/26 01:43:17 [main]: INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)
15/06/26 01:43:17 [main]: INFO log.PerfLogger: </PERFLOG method=compile start=1435263195851 end=1435263197433 duration=1582 from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: INFO metadata.Hive: Dumping metastore api call timing information for : compilation phase
15/06/26 01:43:17 [main]: DEBUG metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(HiveConf, )=2, getAllDatabases_()=19, getFunctions_(String, String, )=122}
15/06/26 01:43:17 [main]: INFO ql.Driver: Concurrency mode is disabled, not creating a lock manager
15/06/26 01:43:17 [main]: INFO log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: INFO ql.Driver: Starting command(queryId=hduser_20150626014315_a00337d7-ec2e-4daf-9ed8-86aa237aab5b): show tables
15/06/26 01:43:17 [main]: INFO log.PerfLogger: </PERFLOG method=TimeToSubmit start=1435263195850 end=1435263197439 duration=1589 from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: INFO log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: INFO log.PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: INFO ql.Driver: Starting task [Stage-0:DDL] in serial mode
15/06/26 01:43:17 [main]: INFO metastore.HiveMetaStore: 0: get_database: default
15/06/26 01:43:17 [main]: INFO HiveMetaStore.audit: ugi=hduser	ip=unknown-ip-addr	cmd=get_database: default	
15/06/26 01:43:17 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.start(ObjectStore.java:2408)
15/06/26 01:43:17 [main]: DEBUG metastore.MetaStoreDirectSql: Direct SQL query in 0.180641ms + 0.025295ms, the query is [SET @@session.sql_mode=ANSI_QUOTES]
15/06/26 01:43:17 [main]: DEBUG metastore.MetaStoreDirectSql: getDatabase: directsql returning db default locn[hdfs://localhost:54310/user/hive/warehouse] desc [Default Hive database] owner [public] ownertype [ROLE]
15/06/26 01:43:17 [main]: DEBUG metastore.ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2445)
15/06/26 01:43:17 [main]: DEBUG metastore.ObjectStore: db details for db default retrieved using SQL in 32.360279ms
15/06/26 01:43:17 [main]: INFO metastore.HiveMetaStore: 0: get_tables: db=default pat=.*
15/06/26 01:43:17 [main]: INFO HiveMetaStore.audit: ugi=hduser	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
15/06/26 01:43:17 [main]: DEBUG metastore.ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTables(ObjectStore.java:944)
15/06/26 01:43:17 [main]: DEBUG metastore.ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTables(ObjectStore.java:972)
15/06/26 01:43:17 [main]: INFO log.PerfLogger: </PERFLOG method=runTasks start=1435263197439 end=1435263197507 duration=68 from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: INFO metadata.Hive: Dumping metastore api call timing information for : execution phase
15/06/26 01:43:17 [main]: DEBUG metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(HiveConf, )=1, getDatabase_(String, )=35, getTables_(String, String, )=13}
15/06/26 01:43:17 [main]: INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1435263197434 end=1435263197508 duration=74 from=org.apache.hadoop.hive.ql.Driver>
OK
15/06/26 01:43:17 [main]: INFO ql.Driver: OK
15/06/26 01:43:17 [main]: INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1435263197509 end=1435263197509 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: INFO log.PerfLogger: </PERFLOG method=Driver.run start=1435263195850 end=1435263197509 duration=1659 from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: INFO Configuration.deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
15/06/26 01:43:17 [main]: DEBUG mapred.FileInputFormat: Time taken to get FileStatuses: 6
15/06/26 01:43:17 [main]: INFO mapred.FileInputFormat: Total input paths to process : 1
15/06/26 01:43:17 [main]: DEBUG mapred.FileInputFormat: Total # of splits generated by getSplits: 1, TimeTaken: 22
15/06/26 01:43:17 [main]: DEBUG exec.FetchOperator: Creating fetchTask with deserializer typeinfo: struct<tab_name:string>
15/06/26 01:43:17 [main]: DEBUG exec.FetchOperator: deserializer properties:
table properties: {columns=tab_name, serialization.null.format= , serialization.lib=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, serialization.format=9, columns.types=string}
partition properties: {columns=tab_name, serialization.null.format= , serialization.lib=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, serialization.format=9, columns.types=string}
saurzcode
user3
15/06/26 01:43:17 [main]: INFO exec.ListSinkOperator: 0 finished. closing... 
15/06/26 01:43:17 [main]: INFO exec.ListSinkOperator: 0 Close done
15/06/26 01:43:17 [main]: DEBUG ql.Driver: Shutting down query show tables
Time taken: 1.665 seconds, Fetched: 2 row(s)
15/06/26 01:43:17 [main]: INFO CliDriver: Time taken: 1.665 seconds, Fetched: 2 row(s)
15/06/26 01:43:17 [main]: INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/06/26 01:43:17 [main]: INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1435263197595 end=1435263197595 duration=0 from=org.apache.hadoop.hive.ql.Driver>
hive> exit;
15/06/26 01:43:26 [main]: DEBUG session.SessionState: Removing resource dir /tmp/2475f092-d0c3-43ca-95b9-b1a3a68a1abc_resources
15/06/26 01:43:26 [main]: DEBUG ipc.Client: The ping interval is 60000 ms.
15/06/26 01:43:26 [main]: DEBUG ipc.Client: Connecting to localhost/127.0.0.1:54310
15/06/26 01:43:26 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser sending #9
15/06/26 01:43:26 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser: starting, having connections 1
15/06/26 01:43:26 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser got value #9
15/06/26 01:43:26 [main]: DEBUG ipc.ProtobufRpcEngine: Call: delete took 21ms
15/06/26 01:43:26 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser sending #10
15/06/26 01:43:26 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser got value #10
15/06/26 01:43:26 [Thread-0]: DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 5ms
15/06/26 01:43:26 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser sending #11
15/06/26 01:43:26 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser got value #11
15/06/26 01:43:26 [Thread-0]: DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 2ms
15/06/26 01:43:26 [Thread-0]: DEBUG ipc.Client: stopping client from cache: org.apache.hadoop.ipc.Client@7e5d4811
15/06/26 01:43:26 [Thread-0]: DEBUG ipc.Client: removing client from cache: org.apache.hadoop.ipc.Client@7e5d4811
15/06/26 01:43:26 [Thread-0]: DEBUG ipc.Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@7e5d4811
15/06/26 01:43:26 [Thread-0]: DEBUG ipc.Client: Stopping client
15/06/26 01:43:26 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser: closed
15/06/26 01:43:26 [IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser]: DEBUG ipc.Client: IPC Client (1707674150) connection to localhost/127.0.0.1:54310 from hduser: stopped, remaining connections 0
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ cd ../../spark-1.2.1/bin/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ./spark-submit ~/sampleHiveSpark.py 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Traceback (most recent call last):
  File "/home/hduser/sampleHiveSpark.py", line 9, in <module>
    sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
  File "/usr/local/spark-1.2.1/python/pyspark/sql.py", line 1620, in sql
    return SchemaRDD(self._ssql_ctx.sql(sqlQuery).toJavaSchemaRDD(), self)
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o19.sql.
: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:235)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:231)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.sql.hive.HiveContext.x$3$lzycompute(HiveContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.x$3(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf$lzycompute(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.<init>(HiveMetastoreCatalog.scala:55)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext$$anon$4.<init>(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:262)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:411)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)
	... 30 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)
	... 35 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)
	... 40 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	... 69 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85)
	... 87 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238)
	... 89 more

hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ./spark-submit /home/arinjoy/Desktop/IITBombay/temp/gender.py 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
PythonRDD[2] at RDD at PythonRDD.scala:43
userId lmsUserId lmsName orgName name gender registrationDate emailId mothertounge highestEduDegree goals city state active firstAccesDate lastAccessDate allowCert yearOfBirth pincode aadharId
(Row(gender=u'f'), 1)
(Row(gender=u''), 1)
(Row(gender=u'm'), 8)
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ./spark-submit --master yarn-cluster --num-executors=19 --class org.apache.spark.examples.sql.hive.HiveFromSpark /home/arinjoy/Desktop/IITBombay/temp/gender.py 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Exception in thread "main" java.lang.Exception: When running with master 'yarn-cluster' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.checkRequiredArguments(SparkSubmitArguments.scala:189)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:82)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:70)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ cd ..
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/spark-submit --jars lib_managed/jars/datanucleus-api-jdo-3.2.6.jar, lib_managed/jars/datanucleus-rdbms-3.2.9.jar, lib_managed/jars/datanucleus-core-3.2.10.jar, lib_managed/jars/lib  /home/arinjoy/Desktop/IITBombay/temp/gender.py 
libfb303-0.9.0.jar   libthrift-0.9.0.jar  
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/spark-submit --jars lib_managed/jars/datanucleus-api-jdo-3.2.6.jar, lib_managed/jars/datanucleus-rdbms-3.2.9.jar, lib_managed/jars/datanucleus-core-3.2.10.jar, lib_managed/jars/m  /home/arinjoy/Desktop/IITBombay/temp/gender.py 
mesos-0.18.1-shaded-protobuf.jar  metrics-core-2.2.0.jar            mockito-all-1.9.0.jar             
metrics-annotation-2.2.0.jar      minlog-1.2.jar                    mqtt-client-0.4.0.jar             
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/spark-submit --jars lib_managed/jars/datanucleus-api-jdo-3.2.6.jar, lib_managed/jars/datanucleus-rdbms-3.2.9.jar, lib_managed/jars/datanucleus-core-3.2.10.jar, ../hive/lib/libmysql-java.jar --files ../hive/conf/hive-site.xml  /home/arinjoy/Desktop/IITBombay/temp/gender.py 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Error: Cannot load main class from JAR file:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar,
Run with --help for usage help or --verbose for debug output
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/spark-submit --jars lib_managed/jars/datanucleus-api-jdo-3.2.6.jar, lib_managed/jars/datanucleus-rdbms-3.2.9.jar, lib_managed/jars/datanucleus-core-3.2.10.jar, ../hive/lib/libmysql-java.jar --files ../hive/conf/hive-site.xml  /home/arinjoy/Desktop/IITBombay/temp/gender.py --verbose
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Error: Cannot load main class from JAR file:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar,
Run with --help for usage help or --verbose for debug output
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/spark-submit --jars lib_managed/jars/datanucleus-api-jdo-3.2.6.jar, lib_managed/jars/datanucleus-core-3.2.10.jar, ../hive/lib/libmysql-java.jar --files ../hive/conf/hive-site.xml  /home/arinjoy/Desktop/IITBombay/temp/gender.py --verbose
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Error: Cannot load main class from JAR file:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-core-3.2.10.jar,
Run with --help for usage help or --verbose for debug output
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/spark-submit --jars lib_managed/jars/datanucleus-api-jdo-3.2.6.jar lib_managed/jars/datanucleus-rdbms-3.2.9.jar lib_managed/jars/datanucleus-core-3.2.10.jar ../hive/lib/libmysql-java.jar --files ../hive/conf/hive-site.xml  /home/arinjoy/Desktop/IITBombay/temp/gender.py --verbose
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Error: No main class set in JAR; please specify one with --class
Run with --help for usage help or --verbose for debug output
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ cd conf/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ s
s: command not found
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ ls
fairscheduler.xml.template  log4j.properties           metrics.properties.template  spark-defaults.conf.template
hive-site.xml               log4j.properties.template  slaves.template              spark-env.sh.template
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ cd ../../hive/conf/
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ ls
beeline-log4j.properties  hive-default.xml  hive-exec-log4j.properties  hive-site.xml    metastore_db
derby.log                 hive-env.sh       hive-log4j.properties       ivysettings.xml
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ vim jpox.properties
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ cd ../../
hduser@arinjoy-Inspiron-3521:/usr/local$ cd spark-1.2
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2$ cd ../spark-1.2.1/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ls
assembly  CONTRIBUTING.md  dev     examples  lib_managed           mllib    project    sbin                   sql        tox.ini
bagel     core             docker  external  LICENSE               network  python     sbt                    streaming  yarn
bin       data             docs    extras    make-distribution.sh  NOTICE   README.md  sbt-launch-0.13.6.jar  target
conf      derby.log        ec2     graphx    metastore_db          pom.xml  repl       scalastyle-config.xml  tools
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ cd metastore_db/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/metastore_db$ ls
log  README_DO_NOT_TOUCH_FILES.txt  seg0  service.properties
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/metastore_db$ cd ..
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/spark-submit ~/sampleHiveSpark.py 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Traceback (most recent call last):
  File "/home/hduser/sampleHiveSpark.py", line 9, in <module>
    sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
  File "/usr/local/spark-1.2.1/python/pyspark/sql.py", line 1620, in sql
    return SchemaRDD(self._ssql_ctx.sql(sqlQuery).toJavaSchemaRDD(), self)
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o19.sql.
: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:235)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:231)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.sql.hive.HiveContext.x$3$lzycompute(HiveContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.x$3(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf$lzycompute(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.<init>(HiveMetastoreCatalog.scala:55)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext$$anon$4.<init>(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:262)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:411)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)
	... 30 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)
	... 35 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)
	... 40 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	... 69 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85)
	... 87 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238)
	... 89 more

hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ $JAVA_HOME 
-su: /usr/lib/jvm/java-7-openjdk-amd64: Is a directory
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ echo $JAVA_HOME 
/usr/lib/jvm/java-7-openjdk-amd64
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ cd ../hive/lib/
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ ls
accumulo-core-1.6.0.jar               datanucleus-api-jdo-3.2.6.jar           jetty-all-server-7.6.0.v20120127.jar
accumulo-fate-1.6.0.jar               datanucleus-core-3.2.10.jar             jline-2.12.jar
accumulo-start-1.6.0.jar              datanucleus-rdbms-3.2.9.jar             joda-time-2.5.jar
accumulo-trace-1.6.0.jar              derby-10.11.1.1.jar                     jpam-1.1.jar
activation-1.1.jar                    eigenbase-properties-1.1.5.jar          json-20090211.jar
ant-1.9.1.jar                         geronimo-annotation_1.0_spec-1.1.1.jar  jsr305-3.0.0.jar
ant-launcher-1.9.1.jar                geronimo-jaspic_1.0_spec-1.0.jar        jta-1.1.jar
antlr-2.7.7.jar                       geronimo-jta_1.1_spec-1.1.1.jar         junit-4.11.jar
antlr-runtime-3.4.jar                 groovy-all-2.1.6.jar                    libfb303-0.9.2.jar
apache-curator-2.6.0.pom              guava-14.0.1.jar                        libmysql-java.jar
apache-log4j-extras-1.2.17.jar        hamcrest-core-1.1.jar                   libthrift-0.9.2.jar
asm-commons-3.1.jar                   hive-accumulo-handler-1.2.0.jar         log4j-1.2.16.jar
asm-tree-3.1.jar                      hive-ant-1.2.0.jar                      mail-1.4.1.jar
avro-1.7.5.jar                        hive-beeline-1.2.0.jar                  maven-scm-api-1.4.jar
bonecp-0.8.0.RELEASE.jar              hive-cli-1.2.0.jar                      maven-scm-provider-svn-commons-1.4.jar
calcite-avatica-1.2.0-incubating.jar  hive-common-1.2.0.jar                   maven-scm-provider-svnexe-1.4.jar
calcite-core-1.2.0-incubating.jar     hive-contrib-1.2.0.jar                  netty-3.7.0.Final.jar
calcite-linq4j-1.2.0-incubating.jar   hive-exec-1.2.0.jar                     opencsv-2.3.jar
commons-beanutils-1.7.0.jar           hive-hbase-handler-1.2.0.jar            oro-2.0.8.jar
commons-beanutils-core-1.8.0.jar      hive-hwi-1.2.0.jar                      paranamer-2.3.jar
commons-cli-1.2.jar                   hive-jdbc-1.2.0.jar                     parquet-hadoop-bundle-1.6.0.jar
commons-codec-1.4.jar                 hive-jdbc-1.2.0-standalone.jar          pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar
commons-collections-3.2.1.jar         hive-metastore-1.2.0.jar                php
commons-compiler-2.7.6.jar            hive-serde-1.2.0.jar                    plexus-utils-1.5.6.jar
commons-compress-1.4.1.jar            hive-service-1.2.0.jar                  py
commons-configuration-1.6.jar         hive-shims-0.20S-1.2.0.jar              regexp-1.3.jar
commons-dbcp-1.4.jar                  hive-shims-0.23-1.2.0.jar               servlet-api-2.5.jar
commons-digester-1.8.jar              hive-shims-1.2.0.jar                    snappy-java-1.0.5.jar
commons-httpclient-3.0.1.jar          hive-shims-common-1.2.0.jar             ST4-4.0.4.jar
commons-io-2.4.jar                    hive-shims-scheduler-1.2.0.jar          stax-api-1.0.1.jar
commons-lang-2.6.jar                  hive-testutils-1.2.0.jar                stringtemplate-3.2.1.jar
commons-logging-1.1.3.jar             httpclient-4.4.jar                      super-csv-2.2.0.jar
commons-math-2.1.jar                  httpcore-4.4.jar                        tempus-fugit-1.1.jar
commons-pool-1.5.4.jar                ivy-2.4.0.jar                           velocity-1.5.jar
commons-vfs2-2.0.jar                  janino-2.7.6.jar                        xz-1.0.jar
curator-client-2.6.0.jar              jcommander-1.32.jar                     zookeeper-3.4.6.jar
curator-framework-2.6.0.jar           jdo-api-3.0.1.jar
curator-recipes-2.6.0.jar             jetty-all-7.6.0.v20120127.jar
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ rm libmysql-java.jar 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ /usr/local/
bin/         games/       hadoop_tmp/  include/     man/         share/       spark-1.2.1/ 
etc/         hadoop/      hive/        lib/         sbin/        spark-1.2/   src/         
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ /usr/lib/jvm/java-7-openjdk-amd64/
bin/     docs/    include/ jre/     lib/     man/     
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ /usr/lib/jvm/java-7-openjdk-amd64/lib/
amd64/ jexec  
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ /usr/lib/jvm/java-7-openjdk-amd64/bin/
appletviewer  jarsigner     javap         jdb           jrunscript    keytool       rmic          servertool    xjc           
apt           java          java-rmi.cgi  jhat          jsadebugd     native2ascii  rmid          tnameserv     
extcheck      javac         javaws        jinfo         jstack        orbd          rmiregistry   unpack200     
idlj          javadoc       jcmd          jmap          jstat         pack200       schemagen     wsgen         
jar           javah         jconsole      jps           jstatd        policytool    serialver     wsimport      
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ /usr/lib/jvm/java-7-openjdk-amd64/jre/
bin/ lib/ man/ 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ /usr/lib/jvm/java-7-openjdk-amd64/jre/lib/
amd64/      cmm/        ext/        images/     jexec       management/ security/   zi/         
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ /usr/share/java
java/        java-config/ javascript/  javazi/      
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ /usr/share/java
java/        java-config/ javascript/  javazi/      
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ cd /usr/share/java/
hduser@arinjoy-Inspiron-3521:/usr/share/java$ ls
AbsoluteLayout-7.0.jar                        easymock-2.4.jar                lucene-collation.jar
AbsoluteLayout.jar                            easymock.jar                    lucene-core-2.9.4.jar
ant-1.8.2.jar                                 ee.foundation-4.2.0.jar         lucene-core.jar
ant-antlr-1.8.2.jar                           ee.foundation.jar               lucene-fast-vector-highlighter-2.9.4.jar
ant-antlr.jar                                 el-api-2.1.jar                  lucene-fast-vector-highlighter.jar
ant-apache-bcel-1.8.2.jar                     emma-2.0.5312+dfsg.jar          lucene-highlighter-2.9.4.jar
ant-apache-bcel.jar                           emma_ant-2.0.5312+dfsg.jar      lucene-highlighter.jar
ant-apache-bsf-1.8.2.jar                      emma_ant.jar                    lucene-instantiated-2.9.4.jar
ant-apache-bsf.jar                            emma.jar                        lucene-instantiated.jar
ant-apache-log4j-1.8.2.jar                    flute-1.1-SNAPSHOT.jar          lucene-lucli-2.9.4.jar
ant-apache-log4j.jar                          flute.jar                       lucene-lucli.jar
ant-apache-oro-1.8.2.jar                      freemarker-2.3.18.jar           lucene-memory-2.9.4.jar
ant-apache-oro.jar                            freemarker.jar                  lucene-memory.jar
ant-apache-regexp-1.8.2.jar                   ganymed-ssh2-250.jar            lucene-misc-2.9.4.jar
ant-apache-regexp.jar                         ganymed-ssh2-build250.jar       lucene-misc.jar
ant-apache-resolver-1.8.2.jar                 ganymed-ssh2.jar                lucene-queries-2.9.4.jar
ant-apache-resolver.jar                       geronimo-jpa-2.0-spec.jar       lucene-queries.jar
ant-apache-xalan2-1.8.2.jar                   geronimo-osgi-locator-1.0.jar   lucene-queryparser-2.9.4.jar
ant-apache-xalan2.jar                         geronimo-osgi-locator.jar       lucene-queryparser.jar
ant-bootstrap.jar                             geronimo-osgi-registry-1.0.jar  lucene-regex-2.9.4.jar
ant-commons-logging-1.8.2.jar                 geronimo-osgi-registry.jar      lucene-regex.jar
ant-commons-logging.jar                       hamcrest-core-1.1.jar           lucene-remote-2.9.4.jar
ant-commons-net-1.8.2.jar                     hamcrest-core.jar               lucene-remote.jar
ant-commons-net.jar                           hamcrest-generator-1.1.jar      lucene-smartcn-2.9.4.jar
ant.jar                                       hamcrest-generator.jar          lucene-smartcn.jar
ant-javamail-1.8.2.jar                        hamcrest-integration-1.1.jar    lucene-snowball-2.9.4.jar
ant-javamail.jar                              hamcrest-integration.jar        lucene-snowball.jar
ant-jdepend-1.8.2.jar                         hamcrest-library-1.1.jar        lucene-spatial-2.9.4.jar
ant-jdepend.jar                               hamcrest-library.jar            lucene-spatial.jar
ant-jmf-1.8.2.jar                             hsqldb-1.8.0.10.jar             lucene-spellchecker-2.9.4.jar
ant-jmf.jar                                   hsqldb.jar                      lucene-spellchecker.jar
ant-jsch-1.8.2.jar                            hsqldbutil-1.8.0.10.jar         lucene-surround-2.9.4.jar
ant-jsch.jar                                  hsqldbutil.jar                  lucene-surround.jar
ant-junit-1.8.2.jar                           icu4j.jar                       lucene-swing-2.9.4.jar
ant-junit4-1.8.2.jar                          ini4j-0.5.2-SNAPSHOT.jar        lucene-swing.jar
ant-junit4.jar                                ini4j.jar                       lucene-wikipedia-2.9.4.jar
ant-junit.jar                                 java-atk-wrapper.jar            lucene-wikipedia.jar
ant-launcher-1.8.2.jar                        javac-api-7.0.1.jar             lucene-wordnet-2.9.4.jar
ant-launcher.jar                              javac-api.jar                   lucene-wordnet.jar
antlr-2.7.7.jar                               javac-impl-7.0.1.jar            lucene-xml-query-parser-2.9.4.jar
antlr3-3.2.jar                                javac-impl.jar                  lucene-xml-query-parser.jar
antlr3.jar                                    java_uno.jar                    mysql-connector-java-5.1.16.jar
antlr3-runtime-3.2.jar                        jaxp-1.4.jar                    mysql-connector-java.jar
antlr3-runtime.jar                            jcl-over-slf4j-1.6.4.jar        mysql.jar
antlrall.jar                                  jcl-over-slf4j.jar              netx-0.5.jar
antlr.debug.jar                               jcodings-1.0.4.jar              netx.jar
antlr.jar                                     jcodings.jar                    org.apache.felix.framework-4.0.1.jar
ant-swing-1.8.2.jar                           jemmy2-2.3.1.1.jar              org.apache.felix.framework.jar
ant-swing.jar                                 jemmy2.jar                      org.apache.felix.main-4.0.1.jar
ant-testutil-1.8.2.jar                        jetty-6.1.24.jar                org.apache.felix.main.jar
ant-testutil.jar                              jetty.jar                       org-openide-modules-7.0.jar
asm3-3.3.2.jar                                jetty-sslengine-6.1.24.jar      org-openide-modules.jar
asm3-all-3.3.2.jar                            jetty-sslengine.jar             org-openide-util-7.0.jar
asm3-all.jar                                  jetty-start-6.1.24.jar          org-openide-util.jar
asm3-analysis-3.3.2.jar                       jetty-start-daemon-6.1.24.jar   org-openide-util-lookup-7.0.jar
asm3-analysis.jar                             jetty-start-daemon.jar          org-openide-util-lookup.jar
asm3-commons-3.3.2.jar                        jetty-start.jar                 oro-2.0.8.jar
asm3-commons.jar                              jetty-util5-6.1.24.jar          oro.jar
asm3-debug-all-3.3.2.jar                      jetty-util5.jar                 osgi.compendium-4.3.0.jar
asm3-debug-all.jar                            jetty-util-6.1.24.jar           osgi.compendium.jar
asm3.jar                                      jetty-util.jar                  osgi.core-4.3.0.jar
asm3-tree-3.3.2.jar                           jh-2.0.05.ds1.jar               osgi.core.jar
asm3-tree.jar                                 jhall-2.0.05.ds1.jar            postgresql.jar
asm3-util-3.3.2.jar                           jhall.jar                       postgresql-jdbc3-9.1.jar
asm3-util.jar                                 jhbasic-2.0.05.ds1.jar          postgresql-jdbc3.jar
asm3-xml-3.3.2.jar                            jhbasic.jar                     postgresql-jdbc4-9.1.jar
asm3-xml.jar                                  jh-client-2.0.05.ds1.jar        postgresql-jdbc4.jar
aspectjrt-1.6.12.jar                          jh-client.jar                   regexp-1.5.jar
aspectjrt.jar                                 jh.jar                          regexp.jar
aspectjtools-1.6.12.jar                       jline-1.0.jar                   ridl.jar
aspectjtools.jar                              jline.jar                       sac-1.3.jar
aspectjweaver-1.6.12.jar                      jna-3.2.7.jar                   sac.jar
aspectjweaver.jar                             jna.jar                         servlet-api-2.5.jar
Azureus2.jar                                  jna-platform-3.2.7.jar          simple-validation-0.4.jar
beansbinding-1.2.1.jar                        jna-platform.jar                simple-validation.jar
beansbinding.jar                              joda-time-1.6.2.jar             slf4j-api-1.6.4.jar
bindex-2.2.jar                                joda-time.jar                   slf4j-api.jar
bindex.jar                                    jsch-0.1.42.jar                 slf4j-jcl-1.6.4.jar
bsaf-1.9.jar                                  jsch.jar                        slf4j-jcl.jar
bsaf.jar                                      jsearch-2.0.05.ds1.jar          slf4j-jdk14-1.6.4.jar
bsh-2.0b4.jar                                 jsearch-client-2.0.05.ds1.jar   slf4j-jdk14.jar
bsh.jar                                       jsearch-client.jar              slf4j-log4j12-1.6.4.jar
bytelist-1.0.6.jar                            jsearch-indexer-2.0.05.ds1.jar  slf4j-log4j12.jar
bytelist.jar                                  jsearch-indexer.jar             slf4j-migrator-1.6.4.jar
cglib-2.2.2.jar                               jsearch.jar                     slf4j-migrator.jar
cglib.jar                                     jsearch-misc-2.0.05.ds1.jar     slf4j-nop-1.6.4.jar
cglib-nodep-2.2.2.jar                         jsearch-misc.jar                slf4j-nop.jar
cglib-nodep.jar                               jsp-api-2.1.jar                 slf4j-simple-1.6.4.jar
com.ibm.icu-4.2.1.1.jar                       jtidy-8.0-alpha-20110807.jar    slf4j-simple.jar
com.ibm.icu.base-4.2.1.1.jar                  jtidy.jar                       stringtemplate-3.2.1.jar
com.ibm.icu.base.jar                          juh.jar                         stringtemplate.jar
com.ibm.icu.jar                               jul-to-slf4j-1.6.4.jar          svnClientAdapter-0.9.100.jar
commons-beanutils-1.8.3.jar                   jul-to-slf4j.jar                svnClientAdapter.jar
commons-beanutils.jar                         junit-3.8.2.jar                 svn-javahl.jar
commons-cli-1.2.jar                           junit4-4.8.2.jar                svnkit-1.3.5.jar
commons-cli.jar                               junit4.jar                      svnkit.jar
commons-codec-1.5.jar                         junit.jar                       svnkit-javahl-1.3.5.jar
commons-codec.jar                             jurt.jar                        svnkit-javahl.jar
commons-collections3-3.2.1.jar                jvyamlb-0.2.5.jar               swing-layout-1.0.4.jar
commons-collections3.jar                      jvyamlb.jar                     swing-layout.jar
commons-collections3-testframework-3.2.1.jar  jzlib-1.1.0.jar                 swingx1-1.0.jar
commons-collections3-testframework.jar        jzlib.jar                       swingx1-beaninfo-1.0.jar
commons-compress-1.2.jar                      libgcj-4.6.3.jar                swingx1-beaninfo.jar
commons-compress.jar                          libgcj-4.6.jar                  swingx1.jar
commons-daemon-1.0.8.jar                      libgcj-tools-4.6.3.jar          swt-gtk-3.7.jar
commons-daemon.jar                            libgcj-tools-4.6.jar            swt.jar
commons-digester-1.8.1.jar                    libintl.jar                     trilead-ssh2-6401.jar
commons-digester.jar                          log4j-1.2-1.2.16.jar            trilead-ssh2.jar
commons-lang-2.6.jar                          log4j-1.2.jar                   unoloader.jar
commons-lang.jar                              log4j-over-slf4j-1.6.4.jar      xercesImpl-2.11.0.jar
commons-logging-1.1.1.jar                     log4j-over-slf4j.jar            xercesImpl.jar
commons-logging-adapters-1.1.1.jar            lucene-analyzers-2.9.4.jar      xercesSamples.jar
commons-logging-adapters.jar                  lucene-analyzers.jar            xml-apis-1.4.01.jar
commons-logging-api-1.1.1.jar                 lucene-ant-2.9.4.jar            xml-apis-ext-1.4.01.jar
commons-logging-api.jar                       lucene-ant.jar                  xml-apis-ext.jar
commons-logging.jar                           lucene-bdb-2.9.4.jar            xml-commons-external-1.4.01.jar
commons-net-1.4.1.jar                         lucene-bdb.jar                  xml-commons-external.jar
commons-net.jar                               lucene-bdb-je-2.9.4.jar         xmlParserAPIs.jar
db-5.1.25.jar                                 lucene-bdb-je.jar               xml-resolver-1.2.jar
db.jar                                        lucene-benchmark-2.9.4.jar      xml-resolver.jar
db-je-3.3.98.jar                              lucene-benchmark.jar
db-je.jar                                     lucene-collation-2.9.4.jar
hduser@arinjoy-Inspiron-3521:/usr/share/java$ sudo cp mysql-connector-java
mysql-connector-java-5.1.16.jar  mysql-connector-java.jar         
hduser@arinjoy-Inspiron-3521:/usr/share/java$ sudo cp mysql-connector-java-5.1.16.jar /usr/local/hive/lib/
[sudo] password for hduser: 
hduser@arinjoy-Inspiron-3521:/usr/share/java$ cd /usr/local/hive/lib/
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ ls
accumulo-core-1.6.0.jar               datanucleus-api-jdo-3.2.6.jar           jetty-all-server-7.6.0.v20120127.jar
accumulo-fate-1.6.0.jar               datanucleus-core-3.2.10.jar             jline-2.12.jar
accumulo-start-1.6.0.jar              datanucleus-rdbms-3.2.9.jar             joda-time-2.5.jar
accumulo-trace-1.6.0.jar              derby-10.11.1.1.jar                     jpam-1.1.jar
activation-1.1.jar                    eigenbase-properties-1.1.5.jar          json-20090211.jar
ant-1.9.1.jar                         geronimo-annotation_1.0_spec-1.1.1.jar  jsr305-3.0.0.jar
ant-launcher-1.9.1.jar                geronimo-jaspic_1.0_spec-1.0.jar        jta-1.1.jar
antlr-2.7.7.jar                       geronimo-jta_1.1_spec-1.1.1.jar         junit-4.11.jar
antlr-runtime-3.4.jar                 groovy-all-2.1.6.jar                    libfb303-0.9.2.jar
apache-curator-2.6.0.pom              guava-14.0.1.jar                        libthrift-0.9.2.jar
apache-log4j-extras-1.2.17.jar        hamcrest-core-1.1.jar                   log4j-1.2.16.jar
asm-commons-3.1.jar                   hive-accumulo-handler-1.2.0.jar         mail-1.4.1.jar
asm-tree-3.1.jar                      hive-ant-1.2.0.jar                      maven-scm-api-1.4.jar
avro-1.7.5.jar                        hive-beeline-1.2.0.jar                  maven-scm-provider-svn-commons-1.4.jar
bonecp-0.8.0.RELEASE.jar              hive-cli-1.2.0.jar                      maven-scm-provider-svnexe-1.4.jar
calcite-avatica-1.2.0-incubating.jar  hive-common-1.2.0.jar                   mysql-connector-java-5.1.16.jar
calcite-core-1.2.0-incubating.jar     hive-contrib-1.2.0.jar                  netty-3.7.0.Final.jar
calcite-linq4j-1.2.0-incubating.jar   hive-exec-1.2.0.jar                     opencsv-2.3.jar
commons-beanutils-1.7.0.jar           hive-hbase-handler-1.2.0.jar            oro-2.0.8.jar
commons-beanutils-core-1.8.0.jar      hive-hwi-1.2.0.jar                      paranamer-2.3.jar
commons-cli-1.2.jar                   hive-jdbc-1.2.0.jar                     parquet-hadoop-bundle-1.6.0.jar
commons-codec-1.4.jar                 hive-jdbc-1.2.0-standalone.jar          pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar
commons-collections-3.2.1.jar         hive-metastore-1.2.0.jar                php
commons-compiler-2.7.6.jar            hive-serde-1.2.0.jar                    plexus-utils-1.5.6.jar
commons-compress-1.4.1.jar            hive-service-1.2.0.jar                  py
commons-configuration-1.6.jar         hive-shims-0.20S-1.2.0.jar              regexp-1.3.jar
commons-dbcp-1.4.jar                  hive-shims-0.23-1.2.0.jar               servlet-api-2.5.jar
commons-digester-1.8.jar              hive-shims-1.2.0.jar                    snappy-java-1.0.5.jar
commons-httpclient-3.0.1.jar          hive-shims-common-1.2.0.jar             ST4-4.0.4.jar
commons-io-2.4.jar                    hive-shims-scheduler-1.2.0.jar          stax-api-1.0.1.jar
commons-lang-2.6.jar                  hive-testutils-1.2.0.jar                stringtemplate-3.2.1.jar
commons-logging-1.1.3.jar             httpclient-4.4.jar                      super-csv-2.2.0.jar
commons-math-2.1.jar                  httpcore-4.4.jar                        tempus-fugit-1.1.jar
commons-pool-1.5.4.jar                ivy-2.4.0.jar                           velocity-1.5.jar
commons-vfs2-2.0.jar                  janino-2.7.6.jar                        xz-1.0.jar
curator-client-2.6.0.jar              jcommander-1.32.jar                     zookeeper-3.4.6.jar
curator-framework-2.6.0.jar           jdo-api-3.0.1.jar
curator-recipes-2.6.0.jar             jetty-all-7.6.0.v20120127.jar
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ cd ../
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ cd ../
hduser@arinjoy-Inspiron-3521:/usr/local$ cd spark-1.2.1/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ls
assembly  CONTRIBUTING.md  dev     examples  lib_managed           mllib    project    sbin                   sql        tox.ini
bagel     core             docker  external  LICENSE               network  python     sbt                    streaming  yarn
bin       data             docs    extras    make-distribution.sh  NOTICE   README.md  sbt-launch-0.13.6.jar  target
conf      derby.log        ec2     graphx    metastore_db          pom.xml  repl       scalastyle-config.xml  tools
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/spark-submit ~/sampleHiveSpark.py 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Traceback (most recent call last):
  File "/home/hduser/sampleHiveSpark.py", line 9, in <module>
    sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
  File "/usr/local/spark-1.2.1/python/pyspark/sql.py", line 1620, in sql
    return SchemaRDD(self._ssql_ctx.sql(sqlQuery).toJavaSchemaRDD(), self)
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o19.sql.
: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:235)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:231)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.sql.hive.HiveContext.x$3$lzycompute(HiveContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.x$3(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf$lzycompute(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.<init>(HiveMetastoreCatalog.scala:55)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext$$anon$4.<init>(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:262)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:411)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)
	... 30 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)
	... 35 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)
	... 40 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	... 69 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85)
	... 87 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238)
	... 89 more

hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ hive
ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory

Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j.properties
hive> show tables;
OK
saurzcode
user3
Time taken: 2.12 seconds, Fetched: 2 row(s)
hive> exit
    > ;
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ cd ../hive/lib/
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ ls
accumulo-core-1.6.0.jar               datanucleus-api-jdo-3.2.6.jar           jetty-all-server-7.6.0.v20120127.jar
accumulo-fate-1.6.0.jar               datanucleus-core-3.2.10.jar             jline-2.12.jar
accumulo-start-1.6.0.jar              datanucleus-rdbms-3.2.9.jar             joda-time-2.5.jar
accumulo-trace-1.6.0.jar              derby-10.11.1.1.jar                     jpam-1.1.jar
activation-1.1.jar                    eigenbase-properties-1.1.5.jar          json-20090211.jar
ant-1.9.1.jar                         geronimo-annotation_1.0_spec-1.1.1.jar  jsr305-3.0.0.jar
ant-launcher-1.9.1.jar                geronimo-jaspic_1.0_spec-1.0.jar        jta-1.1.jar
antlr-2.7.7.jar                       geronimo-jta_1.1_spec-1.1.1.jar         junit-4.11.jar
antlr-runtime-3.4.jar                 groovy-all-2.1.6.jar                    libfb303-0.9.2.jar
apache-curator-2.6.0.pom              guava-14.0.1.jar                        libthrift-0.9.2.jar
apache-log4j-extras-1.2.17.jar        hamcrest-core-1.1.jar                   log4j-1.2.16.jar
asm-commons-3.1.jar                   hive-accumulo-handler-1.2.0.jar         mail-1.4.1.jar
asm-tree-3.1.jar                      hive-ant-1.2.0.jar                      maven-scm-api-1.4.jar
avro-1.7.5.jar                        hive-beeline-1.2.0.jar                  maven-scm-provider-svn-commons-1.4.jar
bonecp-0.8.0.RELEASE.jar              hive-cli-1.2.0.jar                      maven-scm-provider-svnexe-1.4.jar
calcite-avatica-1.2.0-incubating.jar  hive-common-1.2.0.jar                   mysql-connector-java-5.0.8-bin.jar
calcite-core-1.2.0-incubating.jar     hive-contrib-1.2.0.jar                  mysql-connector-java-5.1.16.jar
calcite-linq4j-1.2.0-incubating.jar   hive-exec-1.2.0.jar                     netty-3.7.0.Final.jar
commons-beanutils-1.7.0.jar           hive-hbase-handler-1.2.0.jar            opencsv-2.3.jar
commons-beanutils-core-1.8.0.jar      hive-hwi-1.2.0.jar                      oro-2.0.8.jar
commons-cli-1.2.jar                   hive-jdbc-1.2.0.jar                     paranamer-2.3.jar
commons-codec-1.4.jar                 hive-jdbc-1.2.0-standalone.jar          parquet-hadoop-bundle-1.6.0.jar
commons-collections-3.2.1.jar         hive-metastore-1.2.0.jar                pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar
commons-compiler-2.7.6.jar            hive-serde-1.2.0.jar                    php
commons-compress-1.4.1.jar            hive-service-1.2.0.jar                  plexus-utils-1.5.6.jar
commons-configuration-1.6.jar         hive-shims-0.20S-1.2.0.jar              py
commons-dbcp-1.4.jar                  hive-shims-0.23-1.2.0.jar               regexp-1.3.jar
commons-digester-1.8.jar              hive-shims-1.2.0.jar                    servlet-api-2.5.jar
commons-httpclient-3.0.1.jar          hive-shims-common-1.2.0.jar             snappy-java-1.0.5.jar
commons-io-2.4.jar                    hive-shims-scheduler-1.2.0.jar          ST4-4.0.4.jar
commons-lang-2.6.jar                  hive-testutils-1.2.0.jar                stax-api-1.0.1.jar
commons-logging-1.1.3.jar             httpclient-4.4.jar                      stringtemplate-3.2.1.jar
commons-math-2.1.jar                  httpcore-4.4.jar                        super-csv-2.2.0.jar
commons-pool-1.5.4.jar                ivy-2.4.0.jar                           tempus-fugit-1.1.jar
commons-vfs2-2.0.jar                  janino-2.7.6.jar                        velocity-1.5.jar
curator-client-2.6.0.jar              jcommander-1.32.jar                     xz-1.0.jar
curator-framework-2.6.0.jar           jdo-api-3.0.1.jar                       zookeeper-3.4.6.jar
curator-recipes-2.6.0.jar             jetty-all-7.6.0.v20120127.jar
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ $CLASS_PATH
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ cp mysql-connector-java-5.0.8-bin.jar /usr/local/spark-1.2.1/lib_managed/
bundles/       jars/          maven-plugins/ orbits/        test-jars/     
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ cp mysql-connector-java-5.0.8-bin.jar /usr/local/spark-1.2.1/lib_managed/jars/
Display all 303 possibilities? (y or n)
activation-1.1.jar                                      jasypt-1.9.0.jar
akka-actor_2.10-2.3.4-spark.jar                         JavaEWAH-0.3.2.jar
akka-remote_2.10-2.3.4-spark.jar                        JavaEWAH-0.6.6.jar
akka-slf4j_2.10-2.3.4-spark.jar                         javax.inject-1.jar
akka-testkit_2.10-2.3.4-spark.jar                       jaxb-api-2.2.2.jar
akka-zeromq_2.10-2.3.4-spark.jar                        jaxb-impl-2.2.3-1.jar
algebird-core_2.10-0.8.1.jar                            jbcrypt-0.3m.jar
ant-1.9.1.jar                                           jblas-1.2.3.jar
ant-launcher-1.9.1.jar                                  jcl-over-slf4j-1.7.5.jar
antlr-2.7.7.jar                                         jcodings-1.0.8.jar
antlr-3.2.jar                                           jdo-api-3.0.1.jar
antlr-runtime-3.4.jar                                   jersey-guice-1.9.jar
aopalliance-1.0.jar                                     jets3t-0.7.1.jar
arpack_combined_all-0.1.jar                             jetty-6.1.26.jar
arpack_combined_all-0.1-javadoc.jar                     jetty-continuation-8.1.14.v20131031.jar
asm-3.1.jar                                             jetty-http-8.1.14.v20131031.jar
asm-3.2.jar                                             jetty-io-8.1.14.v20131031.jar
asm-3.3.1.jar                                           jetty-jndi-8.1.14.v20131031.jar
avro-1.7.3.jar                                          jetty-plus-8.1.14.v20131031.jar
avro-1.7.4.jar                                          jetty-security-8.1.14.v20131031.jar
avro-1.7.6.jar                                          jetty-server-8.1.14.v20131031.jar
avro-compiler-1.7.3.jar                                 jetty-servlet-8.1.14.v20131031.jar
avro-ipc-1.7.3.jar                                      jetty-sslengine-6.1.26.jar
avro-ipc-1.7.6.jar                                      jetty-util-6.1.26.jar
avro-ipc-1.7.6-tests.jar                                jetty-util-8.1.14.v20131031.jar
avro-mapred-1.7.6.jar                                   jetty-webapp-8.1.14.v20131031.jar
breeze_2.10-0.10.jar                                    jetty-websocket-8.1.14.v20131031.jar
breeze-macros_2.10-0.3.1.jar                            jetty-xml-8.1.14.v20131031.jar
cassandra-all-1.2.6.jar                                 jline-0.9.94.jar
cassandra-thrift-1.2.6.jar                              jline-2.10.4.jar
cglib-2.2.1-v20090111.jar                               jna-3.0.9.jar
cglib-nodep-2.2.2.jar                                   jna-3.4.0.jar
chill_2.10-0.5.0.jar                                    jnr-constants-0.8.2.jar
chill-java-0.5.0.jar                                    joda-time-2.1.jar
commons-beanutils-1.7.0.jar                             joni-2.1.2.jar
commons-beanutils-core-1.8.0.jar                        jopt-simple-3.2.jar
commons-cli-1.2.jar                                     json-20080701.jar
commons-codec-1.4.jar                                   json-20090211.jar
commons-codec-1.5.jar                                   json4s-ast_2.10-3.2.10.jar
commons-codec-1.6.jar                                   json4s-core_2.10-3.2.10.jar
commons-codec-1.8.jar                                   json4s-jackson_2.10-3.2.10.jar
commons-codec-1.9.jar                                   json-simple-1.1.jar
commons-collections-3.2.1.jar                           jsp-2.1-6.1.14.jar
commons-compress-1.4.1.jar                              jsp-api-2.1-6.1.14.jar
commons-configuration-1.6.jar                           jsr305-1.3.9.jar
commons-digester-1.8.jar                                jta-1.1.jar
commons-el-1.0.jar                                      jtransforms-2.4.0.jar
commons-exec-1.1.jar                                    jul-to-slf4j-1.7.5.jar
commons-httpclient-3.1.jar                              junit-4.10.jar
commons-io-2.1.jar                                      junit-4.11.jar
commons-io-2.4.jar                                      junit-dep-4.10.jar
commons-lang-2.4.jar                                    junit-dep-4.8.2.jar
commons-lang-2.6.jar                                    junit-interface-0.10.jar
commons-lang3-3.3.2.jar                                 junit-interface-0.9.jar
commons-logging-1.1.1.jar                               kafka_2.10-0.8.0.jar
commons-logging-1.1.3.jar                               libfb303-0.9.0.jar
commons-math-2.1.jar                                    libthrift-0.9.0.jar
commons-math3-3.1.1.jar                                 lz4-1.2.0.jar
commons-net-2.2.jar                                     mesos-0.18.1-shaded-protobuf.jar
commons-net-3.1.jar                                     metrics-annotation-2.2.0.jar
core-1.1.2.jar                                          metrics-core-2.2.0.jar
cssparser-0.9.13.jar                                    minlog-1.2.jar
datanucleus-api-jdo-3.2.6.jar                           mockito-all-1.9.0.jar
datanucleus-core-3.2.10.jar                             mqtt-client-0.4.0.jar
datanucleus-rdbms-3.2.9.jar                             nekohtml-1.9.20.jar
derby-10.10.1.1.jar                                     netty-all-4.0.23.Final.jar
easymock-3.1.jar                                        objenesis-1.2.jar
easymockclassextension-3.1.jar                          opencsv-2.3.jar
findbugs-annotations-1.3.9-1.jar                        oro-2.0.8.jar
flume-ng-configuration-1.4.0.jar                        paradise_2.10.4-2.0.1.jar
flume-ng-core-1.4.0.jar                                 paranamer-2.3.jar
flume-ng-sdk-1.4.0.jar                                  paranamer-2.6.jar
genjavadoc-plugin_2.10.4-0.8.jar                        parquet-column-1.6.0rc3.jar
geronimo-j2ee-management_1.1_spec-1.0.1.jar             parquet-common-1.6.0rc3.jar
geronimo-jms_1.1_spec-1.1.1.jar                         parquet-encoding-1.6.0rc3.jar
groovy-all-2.1.6.jar                                    parquet-format-2.2.0-rc1.jar
gson-2.2.2.jar                                          parquet-generator-1.6.0rc3.jar
gson-2.2.4.jar                                          parquet-hadoop-1.6.0rc3.jar
guava-10.0.1.jar                                        parquet-hadoop-bundle-1.3.2.jar
guice-3.0.jar                                           parquet-jackson-1.6.0rc3.jar
guice-servlet-3.0.jar                                   platform-3.4.0.jar
hadoop-annotations-2.6.0.jar                            py4j-0.8.2.1.jar
hadoop-auth-2.6.0.jar                                   pyrolite-2.0.1.jar
hadoop-client-1.0.4.jar                                 quasiquotes_2.10-2.0.1.jar
hadoop-client-2.6.0.jar                                 reflectasm-1.07-shaded.jar
hadoop-common-2.6.0.jar                                 sac-1.3.jar
hadoop-core-1.0.4.jar                                   scalacheck_2.10-1.11.3.jar
hadoop-hdfs-2.6.0.jar                                   scala-compiler-2.10.0.jar
hadoop-mapreduce-client-app-2.6.0.jar                   scala-compiler-2.10.1.jar
hadoop-mapreduce-client-common-2.6.0.jar                scala-compiler-2.10.4.jar
hadoop-mapreduce-client-core-2.6.0.jar                  scala-library-2.10.4.jar
hadoop-mapreduce-client-jobclient-2.6.0.jar             scalap-2.10.0.jar
hadoop-mapreduce-client-shuffle-2.6.0.jar               scala-reflect-2.10.0.jar
hadoop-yarn-api-2.6.0.jar                               scala-reflect-2.10.1.jar
hadoop-yarn-client-2.6.0.jar                            scala-reflect-2.10.4.jar
hadoop-yarn-common-2.6.0.jar                            scopt_2.10-3.2.0.jar
hadoop-yarn-server-applicationhistoryservice-2.6.0.jar  selenium-api-2.42.2.jar
hadoop-yarn-server-common-2.6.0.jar                     selenium-chrome-driver-2.42.2.jar
hadoop-yarn-server-nodemanager-2.6.0.jar                selenium-firefox-driver-2.42.2.jar
hadoop-yarn-server-resourcemanager-2.6.0.jar            selenium-htmlunit-driver-2.42.2.jar
hadoop-yarn-server-tests-2.6.0-tests.jar                selenium-ie-driver-2.42.2.jar
hadoop-yarn-server-web-proxy-2.6.0.jar                  selenium-java-2.42.2.jar
hamcrest-core-1.1.jar                                   selenium-remote-driver-2.42.2.jar
hamcrest-core-1.3.jar                                   selenium-safari-driver-2.42.2.jar
hbase-client-0.98.7-hadoop1.jar                         selenium-support-2.42.2.jar
hbase-common-0.98.7-hadoop1.jar                         serializer-2.7.1.jar
hbase-hadoop1-compat-0.98.7-hadoop1.jar                 servlet-api-2.5-20081211.jar
hbase-hadoop-compat-0.98.7-hadoop1.jar                  servlet-api-2.5-20110124.jar
hbase-prefix-tree-0.98.7-hadoop1.jar                    servlet-api-2.5-6.1.14.jar
hbase-protocol-0.98.7-hadoop1.jar                       slf4j-api-1.6.4.jar
hbase-server-0.98.7-hadoop1.jar                         slf4j-api-1.7.5.jar
hbase-testing-util-0.98.7-hadoop1.jar                   slf4j-api-1.7.6.jar
high-scale-lib-1.1.2.jar                                slf4j-log4j12-1.6.1.jar
hive-ant-0.13.1a.jar                                    slf4j-log4j12-1.7.5.jar
hive-common-0.13.1a.jar                                 snakeyaml-1.6.jar
hive-exec-0.13.1a.jar                                   snappy-0.2.jar
hive-metastore-0.13.1a.jar                              snaptree-0.1.jar
hive-serde-0.13.1a.jar                                  spire_2.10-0.7.4.jar
hive-shims-0.13.1a.jar                                  spire-macros_2.10-0.7.4.jar
hive-shims-0.20-0.13.1a.jar                             spring-aop-3.0.7.RELEASE.jar
hive-shims-0.20S-0.13.1a.jar                            spring-asm-3.0.7.RELEASE.jar
hive-shims-0.23-0.13.1a.jar                             spring-beans-3.0.7.RELEASE.jar
hive-shims-common-0.13.1a.jar                           spring-context-3.0.7.RELEASE.jar
hive-shims-common-secure-0.13.1a.jar                    spring-core-3.0.7.RELEASE.jar
hsqldb-1.8.0.10.jar                                     spring-expression-3.0.7.RELEASE.jar
htmlunit-2.14.jar                                       ST4-4.0.4.jar
htmlunit-core-js-2.14.jar                               stax-api-1.0.1.jar
htrace-core-2.04.jar                                    stax-api-1.0-2.jar
htrace-core-3.0.4.jar                                   stream-2.7.0.jar
httpclient-4.2.5.jar                                    stringtemplate-3.2.1.jar
httpclient-4.3.2.jar                                    tachyon-0.5.0.jar
httpcore-4.2.4.jar                                      tachyon-client-0.5.0.jar
httpcore-4.2.5.jar                                      test-interface-0.5.jar
httpcore-4.3.1.jar                                      test-interface-1.0.jar
httpmime-4.3.2.jar                                      twitter4j-core-3.0.3.jar
jackson-core-asl-1.0.1.jar                              twitter4j-stream-3.0.3.jar
jackson-core-asl-1.9.11.jar                             uncommons-maths-1.2.2a.jar
jackson-core-asl-1.9.13.jar                             unused-1.0.0.jar
jackson-core-asl-1.9.3.jar                              velocity-1.7.jar
jackson-jaxrs-1.8.8.jar                                 webbit-0.4.14.jar
jackson-jaxrs-1.9.13.jar                                xalan-2.7.1.jar
jackson-mapper-asl-1.0.1.jar                            xercesImpl-2.11.0.jar
jackson-mapper-asl-1.9.11.jar                           xercesImpl-2.9.1.jar
jackson-mapper-asl-1.9.13.jar                           xml-apis-1.3.04.jar
jackson-mapper-asl-1.9.3.jar                            xml-apis-1.4.01.jar
jackson-xc-1.7.1.jar                                    xmlenc-0.52.jar
jackson-xc-1.9.13.jar                                   xz-1.0.jar
jamm-0.2.5.jar                                          zeromq-scala-binding_2.10-0.0.7-spark.jar
jamon-runtime-2.3.1.jar                                 zkclient-0.3.jar
jansi-1.4.jar                                           zookeeper-3.4.5.jar
jasper-compiler-5.5.23.jar                              zookeeper-3.4.6.jar
jasper-runtime-5.5.23.jar                               
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ cp mysql-connector-java-5.0.8-bin.jar /usr/local/spark-1.2.1/lib_managed/jars/
cp: cannot create regular file `/usr/local/spark-1.2.1/lib_managed/jars/mysql-connector-java-5.0.8-bin.jar': Permission denied
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ sudo cp mysql-connector-java-5.0.8-bin.jar /usr/local/spark-1.2.1/lib_managed/jars/
[sudo] password for hduser: 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ ./bin/spark-submit ~/sampleHiveSpark.py 
-su: ./bin/spark-submit: No such file or directory
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ cd ../../spark-1.2.1/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/spark-submit ~/sampleHiveSpark.py 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Traceback (most recent call last):
  File "/home/hduser/sampleHiveSpark.py", line 9, in <module>
    sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
  File "/usr/local/spark-1.2.1/python/pyspark/sql.py", line 1620, in sql
    return SchemaRDD(self._ssql_ctx.sql(sqlQuery).toJavaSchemaRDD(), self)
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o19.sql.
: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:235)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:231)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.sql.hive.HiveContext.x$3$lzycompute(HiveContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.x$3(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf$lzycompute(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.<init>(HiveMetastoreCatalog.scala:55)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext$$anon$4.<init>(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:262)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:411)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)
	... 30 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)
	... 35 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)
	... 40 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	... 69 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85)
	... 87 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238)
	... 89 more

hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ exit
logout
arinjoy@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ su - hduser
Password: 
hduser@arinjoy-Inspiron-3521:~$ cd /usr/local/spark-1.2.1/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./bin/spark-submit ~/sampleHiveSpark.py 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Traceback (most recent call last):
  File "/home/hduser/sampleHiveSpark.py", line 9, in <module>
    sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
  File "/usr/local/spark-1.2.1/python/pyspark/sql.py", line 1620, in sql
    return SchemaRDD(self._ssql_ctx.sql(sqlQuery).toJavaSchemaRDD(), self)
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o19.sql.
: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:235)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:231)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.sql.hive.HiveContext.x$3$lzycompute(HiveContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.x$3(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf$lzycompute(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.<init>(HiveMetastoreCatalog.scala:55)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext$$anon$4.<init>(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:262)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:411)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)
	... 30 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)
	... 35 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)
	... 40 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	... 69 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85)
	... 87 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238)
	... 89 more

hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ mysql-server --v
mysql-server: command not found
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ mysql-server
mysql-server: command not found
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ sudo apt-get install mysql-server
Reading package lists... Done
Building dependency tree       
Reading state information... Done
mysql-server is already the newest version.
0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ mysql --v
mysql: ambiguous option '--v' (vertical, version)
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ mysql --version
mysql  Ver 14.14 Distrib 5.5.43, for debian-linux-gnu (x86_64) using readline 6.2
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ whereis my.cnf
my:
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ mysql
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 209
Server version: 5.5.43-0ubuntu0.12.04.1 (Ubuntu)

Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show table status;
ERROR 1046 (3D000): No database selected
mysql> show database;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'database' at line 1
mysql> show database
    -> ;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'database' at line 1
mysql> exit;
Bye
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ $JAVA_HOME
-su: /usr/lib/jvm/java-7-openjdk-amd64: Is a directory
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ cd conf/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ ls
fairscheduler.xml.template  log4j.properties           metrics.properties.template  spark-defaults.conf.template
hive-site.xml               log4j.properties.template  slaves.template              spark-env.sh.template
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ vi spark-env.sh.template 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ cd ../bin/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ls
beeline                derby.log          pyspark       run-example2.cmd  spark-class.cmd   spark-sql          utils.sh
beeline.cmd            load-spark-env.sh  pyspark2.cmd  run-example.cmd   spark-shell       spark-submit
compute-classpath.cmd  metastore_db       pyspark.cmd   spark-class       spark-shell2.cmd  spark-submit2.cmd
compute-classpath.sh   myUser.csv         run-example   spark-class2.cmd  spark-shell.cmd   spark-submit.cmd
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ pyspark
pyspark: command not found
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ./pyspark
Python 2.7.3 (default, Dec 18 2014, 19:10:20) 
[GCC 4.6.3] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.2.1
      /_/

Using Python version 2.7.3 (default, Dec 18 2014 19:10:20)
SparkContext available as sc.
>>> from pyspark.sql import HiveContext
>>> sqlContext = HiveContext(sc)
>>> sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/spark-1.2.1/python/pyspark/sql.py", line 1620, in sql
    return SchemaRDD(self._ssql_ctx.sql(sqlQuery).toJavaSchemaRDD(), self)
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o17.sql.
: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:235)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:231)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.sql.hive.HiveContext.x$3$lzycompute(HiveContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.x$3(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf$lzycompute(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.<init>(HiveMetastoreCatalog.scala:55)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext$$anon$4.<init>(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:262)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:411)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)
	... 30 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)
	... 35 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)
	... 40 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	... 69 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85)
	... 87 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238)
	... 89 more

>>> EXIT()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'EXIT' is not defined
>>> exit()
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ cd ../../spark-1.2
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2$ sbt 
~/             CHANGES.txt    data/          ec2/           lib/           metastore_db/  python/        RELEASE        wc_output.txt/
bin/           conf/          derby.log      examples/      LICENSE        NOTICE         README.md      sbin/          wc_out.txt/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2$ ./bin/spark-submit ~/sampleHiveSpark.py 
Traceback (most recent call last):
  File "/home/hduser/sampleHiveSpark.py", line 3, in <module>
    from pyspark import SparkContext
  File "/usr/local/spark-1.2/python/pyspark/__init__.py", line 52, in <module>
    from pyspark.sql import SQLContext, HiveContext, SchemaRDD, Row
  File "/usr/local/spark-1.2/python/pyspark/sql/__init__.py", line 41, in <module>
    from pyspark.sql.context import SQLContext, HiveContext
  File "/usr/local/spark-1.2/python/pyspark/sql/context.py", line 32, in <module>
    import pandas
  File "/usr/local/lib/python2.7/dist-packages/pandas/__init__.py", line 39, in <module>
    from pandas.core.api import *
  File "/usr/local/lib/python2.7/dist-packages/pandas/core/api.py", line 12, in <module>
    from pandas.core.series import Series, TimeSeries
  File "/usr/local/lib/python2.7/dist-packages/pandas/core/series.py", line 2606, in <module>
    import pandas.tools.plotting as _gfx
  File "/usr/local/lib/python2.7/dist-packages/pandas/tools/plotting.py", line 23, in <module>
    import pandas.tseries.converter as conv
  File "/usr/local/lib/python2.7/dist-packages/pandas/tseries/converter.py", line 7, in <module>
    import matplotlib.units as units
  File "/usr/lib/pymodules/python2.7/matplotlib/__init__.py", line 774, in <module>
    rcParams = rc_params()
  File "/usr/lib/pymodules/python2.7/matplotlib/__init__.py", line 692, in rc_params
    fname = matplotlib_fname()
  File "/usr/lib/pymodules/python2.7/matplotlib/__init__.py", line 604, in matplotlib_fname
    fname = os.path.join(get_configdir(), 'matplotlibrc')
  File "/usr/lib/pymodules/python2.7/matplotlib/__init__.py", line 253, in wrapper
    ret = func(*args, **kwargs)
  File "/usr/lib/pymodules/python2.7/matplotlib/__init__.py", line 475, in _get_configdir
    raise RuntimeError("'%s' is not a writable dir; you must set %s/.matplotlib to be a writable dir.  You can also set environment variable MPLCONFIGDIR to any writable directory where you want matplotlib data stored "% (h, h))
RuntimeError: '/home/hduser' is not a writable dir; you must set /home/hduser/.matplotlib to be a writable dir.  You can also set environment variable MPLCONFIGDIR to any writable directory where you want matplotlib data stored 
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2$ sudo ./bin/spark-submit ~/sampleHiveSpark.py 
[sudo] password for hduser: 
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2$ ls
~    CHANGES.txt  data       ec2       lib      metastore_db  python     RELEASE  wc_output.txt
bin  conf         derby.log  examples  LICENSE  NOTICE        README.md  sbin     wc_out.txt
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2$ ./pyspark
-su: ./pyspark: No such file or directory
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2$ ./bin/pyspark
Python 2.7.3 (default, Dec 18 2014, 19:10:20) 
[GCC 4.6.3] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Traceback (most recent call last):
  File "/usr/local/spark-1.2/python/pyspark/shell.py", line 37, in <module>
    import pyspark
  File "/usr/local/spark-1.2/python/pyspark/__init__.py", line 52, in <module>
    from pyspark.sql import SQLContext, HiveContext, SchemaRDD, Row
  File "/usr/local/spark-1.2/python/pyspark/sql/__init__.py", line 41, in <module>
    from pyspark.sql.context import SQLContext, HiveContext
  File "/usr/local/spark-1.2/python/pyspark/sql/context.py", line 32, in <module>
    import pandas
  File "/usr/local/lib/python2.7/dist-packages/pandas/__init__.py", line 39, in <module>
    from pandas.core.api import *
  File "/usr/local/lib/python2.7/dist-packages/pandas/core/api.py", line 12, in <module>
    from pandas.core.series import Series, TimeSeries
  File "/usr/local/lib/python2.7/dist-packages/pandas/core/series.py", line 2606, in <module>
    import pandas.tools.plotting as _gfx
  File "/usr/local/lib/python2.7/dist-packages/pandas/tools/plotting.py", line 23, in <module>
    import pandas.tseries.converter as conv
  File "/usr/local/lib/python2.7/dist-packages/pandas/tseries/converter.py", line 7, in <module>
    import matplotlib.units as units
  File "/usr/lib/pymodules/python2.7/matplotlib/__init__.py", line 774, in <module>
    rcParams = rc_params()
  File "/usr/lib/pymodules/python2.7/matplotlib/__init__.py", line 692, in rc_params
    fname = matplotlib_fname()
  File "/usr/lib/pymodules/python2.7/matplotlib/__init__.py", line 604, in matplotlib_fname
    fname = os.path.join(get_configdir(), 'matplotlibrc')
  File "/usr/lib/pymodules/python2.7/matplotlib/__init__.py", line 253, in wrapper
    ret = func(*args, **kwargs)
  File "/usr/lib/pymodules/python2.7/matplotlib/__init__.py", line 475, in _get_configdir
    raise RuntimeError("'%s' is not a writable dir; you must set %s/.matplotlib to be a writable dir.  You can also set environment variable MPLCONFIGDIR to any writable directory where you want matplotlib data stored "% (h, h))
RuntimeError: '/home/hduser' is not a writable dir; you must set /home/hduser/.matplotlib to be a writable dir.  You can also set environment variable MPLCONFIGDIR to any writable directory where you want matplotlib data stored 
>>> exit;
Use exit() or Ctrl-D (i.e. EOF) to exit
>>> 
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2$ sudo ./bin/pyspark
Python 2.7.3 (default, Dec 18 2014, 19:10:20) 
[GCC 4.6.3] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.3.1
      /_/

Using Python version 2.7.3 (default, Dec 18 2014 19:10:20)
SparkContext available as sc, HiveContext available as sqlContext.
>>> exit;
Use exit() or Ctrl-D (i.e. EOF) to exit
>>> 
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2$ ls
~    CHANGES.txt  data       ec2       lib      metastore_db  python     RELEASE  wc_output.txt
bin  conf         derby.log  examples  LICENSE  NOTICE        README.md  sbin     wc_out.txt
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2$ cd conf/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2/conf$ ls
fairscheduler.xml.template  log4j.properties.template    slaves.template               spark-env.sh.template
log4j.properties            metrics.properties.template  spark-defaults.conf.template
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2/conf$ vim spark-env.sh.template 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2/conf$ vim spark-defaults.conf.template 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2/conf$ cd ../../spark-1.2.1/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ls
assembly  CONTRIBUTING.md  dev     examples  lib_managed           mllib    project    sbin                   sql        tox.ini
bagel     core             docker  external  LICENSE               network  python     sbt                    streaming  yarn
bin       data             docs    extras    make-distribution.sh  NOTICE   README.md  sbt-launch-0.13.6.jar  target
conf      derby.log        ec2     graphx    metastore_db          pom.xml  repl       scalastyle-config.xml  tools
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./sb
sbin/ sbt/  
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ ./sbin/start-thriftserver.sh 
mkdir: cannot create directory `/usr/local/spark-1.2.1/sbin/../logs': Permission denied
chown: cannot access `/usr/local/spark-1.2.1/sbin/../logs': No such file or directory
starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /usr/local/spark-1.2.1/sbin/../logs/spark-hduser-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-arinjoy-Inspiron-3521.out
/usr/local/spark-1.2.1/sbin/spark-daemon.sh: line 148: /usr/local/spark-1.2.1/sbin/../logs/spark-hduser-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-arinjoy-Inspiron-3521.out: No such file or directory
failed to launch org.apache.spark.sql.hive.thriftserver.HiveThriftServer2:
tail: cannot open `/usr/local/spark-1.2.1/sbin/../logs/spark-hduser-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-arinjoy-Inspiron-3521.out' for reading: No such file or directory
full log in /usr/local/spark-1.2.1/sbin/../logs/spark-hduser-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-arinjoy-Inspiron-3521.out
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ jps
29043 DataNode
29784 NodeManager
3796 Jps
29536 ResourceManager
29328 SecondaryNameNode
28800 NameNode
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ stop-all.sh
This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh
Stopping namenodes on [localhost]
localhost: stopping namenode
localhost: stopping datanode


^Z
[1]+  Stopped                 stop-all.sh
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ cd bin
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ls
beeline                derby.log          pyspark       run-example2.cmd  spark-class.cmd   spark-sql          utils.sh
beeline.cmd            load-spark-env.sh  pyspark2.cmd  run-example.cmd   spark-shell       spark-submit
compute-classpath.cmd  metastore_db       pyspark.cmd   spark-class       spark-shell2.cmd  spark-submit2.cmd
compute-classpath.sh   myUser.csv         run-example   spark-class2.cmd  spark-shell.cmd   spark-submit.cmd
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ./compute-classpath.sh 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
::/usr/local/spark-1.2.1/conf:/usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.6.0.jar:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-core-3.2.10.jar
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ vi compute-classpath.sh
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ sudo vi compute-classpath.sh
[sudo] password for hduser: 
Sorry, try again.
[sudo] password for hduser: 
Sorry, try again.
[sudo] password for hduser: 
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ stop-all.sh 
This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh
Stopping namenodes on [localhost]
localhost: no namenode to stop
localhost: no datanode to stop
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode
stopping yarn daemons
stopping resourcemanager
resourcemanager did not stop gracefully after 5 seconds: killing with kill -9
localhost: stopping nodemanager
localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9
no proxyserver to stop
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ start-all.sh 
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on [localhost]
localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hduser-namenode-arinjoy-Inspiron-3521.out
localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hduser-datanode-arinjoy-Inspiron-3521.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hduser-secondarynamenode-arinjoy-Inspiron-3521.out
starting yarn daemons
starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-arinjoy-Inspiron-3521.out
localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-arinjoy-Inspiron-3521.out
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ./compute-classpath.sh 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
::/usr/local/spark-1.2.1/conf:/usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.6.0.jar:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-core-3.2.10.jar
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ sudo vi compute-classpath.sh
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ./compute-classpath.sh 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
::/usr/local/spark-1.2.1/conf:/usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.6.0.jar:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-core-3.2.10.jar
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ sudo vi compute-classpath.sh
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ./compute-classpath.sh 
Spark assembly has been built with Hive, including Datanucleus jars on classpath
::/usr/local/spark-1.2.1/conf:/usr/local/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.6.0.jar:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/usr/local/spark-1.2.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/usr/local/spark-1.2.1/lib_managed/jars/mysql-connector-java-5.0.8-bin.jar
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ cd ../
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ sudo ./bin/spark-submit ~/sampleHiveSpark.py 
Loading Spark jar with 'jar' failed. 
This is likely because Spark was compiled with Java 7 and run 
with Java 6. (see SPARK-1703). Please use Java 7 to run Spark 
or build Spark with Java 6.

hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ cd ../hive/lib
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ ls
accumulo-core-1.6.0.jar               datanucleus-api-jdo-3.2.6.jar           jetty-all-server-7.6.0.v20120127.jar
accumulo-fate-1.6.0.jar               datanucleus-core-3.2.10.jar             jline-2.12.jar
accumulo-start-1.6.0.jar              datanucleus-rdbms-3.2.9.jar             joda-time-2.5.jar
accumulo-trace-1.6.0.jar              derby-10.11.1.1.jar                     jpam-1.1.jar
activation-1.1.jar                    eigenbase-properties-1.1.5.jar          json-20090211.jar
ant-1.9.1.jar                         geronimo-annotation_1.0_spec-1.1.1.jar  jsr305-3.0.0.jar
ant-launcher-1.9.1.jar                geronimo-jaspic_1.0_spec-1.0.jar        jta-1.1.jar
antlr-2.7.7.jar                       geronimo-jta_1.1_spec-1.1.1.jar         junit-4.11.jar
antlr-runtime-3.4.jar                 groovy-all-2.1.6.jar                    libfb303-0.9.2.jar
apache-curator-2.6.0.pom              guava-14.0.1.jar                        libthrift-0.9.2.jar
apache-log4j-extras-1.2.17.jar        hamcrest-core-1.1.jar                   log4j-1.2.16.jar
asm-commons-3.1.jar                   hive-accumulo-handler-1.2.0.jar         mail-1.4.1.jar
asm-tree-3.1.jar                      hive-ant-1.2.0.jar                      maven-scm-api-1.4.jar
avro-1.7.5.jar                        hive-beeline-1.2.0.jar                  maven-scm-provider-svn-commons-1.4.jar
bonecp-0.8.0.RELEASE.jar              hive-cli-1.2.0.jar                      maven-scm-provider-svnexe-1.4.jar
calcite-avatica-1.2.0-incubating.jar  hive-common-1.2.0.jar                   mysql-connector-java-5.0.8-bin.jar
calcite-core-1.2.0-incubating.jar     hive-contrib-1.2.0.jar                  mysql-connector-java-5.1.16.jar
calcite-linq4j-1.2.0-incubating.jar   hive-exec-1.2.0.jar                     netty-3.7.0.Final.jar
commons-beanutils-1.7.0.jar           hive-hbase-handler-1.2.0.jar            opencsv-2.3.jar
commons-beanutils-core-1.8.0.jar      hive-hwi-1.2.0.jar                      oro-2.0.8.jar
commons-cli-1.2.jar                   hive-jdbc-1.2.0.jar                     paranamer-2.3.jar
commons-codec-1.4.jar                 hive-jdbc-1.2.0-standalone.jar          parquet-hadoop-bundle-1.6.0.jar
commons-collections-3.2.1.jar         hive-metastore-1.2.0.jar                pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar
commons-compiler-2.7.6.jar            hive-serde-1.2.0.jar                    php
commons-compress-1.4.1.jar            hive-service-1.2.0.jar                  plexus-utils-1.5.6.jar
commons-configuration-1.6.jar         hive-shims-0.20S-1.2.0.jar              py
commons-dbcp-1.4.jar                  hive-shims-0.23-1.2.0.jar               regexp-1.3.jar
commons-digester-1.8.jar              hive-shims-1.2.0.jar                    servlet-api-2.5.jar
commons-httpclient-3.0.1.jar          hive-shims-common-1.2.0.jar             snappy-java-1.0.5.jar
commons-io-2.4.jar                    hive-shims-scheduler-1.2.0.jar          ST4-4.0.4.jar
commons-lang-2.6.jar                  hive-testutils-1.2.0.jar                stax-api-1.0.1.jar
commons-logging-1.1.3.jar             httpclient-4.4.jar                      stringtemplate-3.2.1.jar
commons-math-2.1.jar                  httpcore-4.4.jar                        super-csv-2.2.0.jar
commons-pool-1.5.4.jar                ivy-2.4.0.jar                           tempus-fugit-1.1.jar
commons-vfs2-2.0.jar                  janino-2.7.6.jar                        velocity-1.5.jar
curator-client-2.6.0.jar              jcommander-1.32.jar                     xz-1.0.jar
curator-framework-2.6.0.jar           jdo-api-3.0.1.jar                       zookeeper-3.4.6.jar
curator-recipes-2.6.0.jar             jetty-all-7.6.0.v20120127.jar
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ rm ../../spark-1.2.1/lib_managed/jars/mysql-connector-java-5.0.8-bin.jar 
rm: remove write-protected regular file `../../spark-1.2.1/lib_managed/jars/mysql-connector-java-5.0.8-bin.jar'? y
rm: cannot remove `../../spark-1.2.1/lib_managed/jars/mysql-connector-java-5.0.8-bin.jar': Permission denied
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ sudo rm ../../spark-1.2.1/lib_managed/jars/mysql-connector-java-5.0.8-bin.jar 
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ sudo cp mysql-connector-java-5.1.16.jar ../../spark-1.2.1/lib_managed/jars/
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ sudo ./bin/spark-submit ~/sampleHiveSpark.py 
sudo: ./bin/spark-submit: command not found
hduser@arinjoy-Inspiron-3521:/usr/local/hive/lib$ cd ../../spark-1.2.1/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ sudo ./bin/spark-submit ~/sampleHiveSpark.py 
Loading Spark jar with 'jar' failed. 
This is likely because Spark was compiled with Java 7 and run 
with Java 6. (see SPARK-1703). Please use Java 7 to run Spark 
or build Spark with Java 6.

hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ cd bin/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ vim compute-classpath.sh 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ sudo vim compute-classpath.sh 
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ sudo ./bin/spark-submit ~/sampleHiveSpark.py 
sudo: ./bin/spark-submit: command not found
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ cd ..
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ sudo ./bin/spark-submit ~/sampleHiveSpark.py 
Loading Spark jar with 'jar' failed. 
This is likely because Spark was compiled with Java 7 and run 
with Java 6. (see SPARK-1703). Please use Java 7 to run Spark 
or build Spark with Java 6.

hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ . ./bin/compute-classpath.sh 
dirname: invalid option -- 's'
Try `dirname --help' for more information.
-su: //bin/load-spark-env.sh: No such file or directory
Failed to find Spark assembly in //assembly/target/scala-
You need to build Spark before running this program.
arinjoy@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ su - hduser
Password: 
hduser@arinjoy-Inspiron-3521:~$ cd /usr/local/hive/
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ hive
ls: cannot access /usr/local/spark-1.2.1/lib/spark-assembly-*.jar: No such file or directory

Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j.properties
hive> show tables;
OK
saurzcode
user3
Time taken: 1.668 seconds, Fetched: 2 row(s)
hive> exit;
hduser@arinjoy-Inspiron-3521:/usr/local/hive$ cd ../spark-1.2.1/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1$ cd bin/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ ./pyspark 
Python 2.7.3 (default, Dec 18 2014, 19:10:20) 
[GCC 4.6.3] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.2.1
      /_/

Using Python version 2.7.3 (default, Dec 18 2014 19:10:20)
SparkContext available as sc.
>>> from pyspark.sql import HiveContext
>>> sqlContext = HiveContext(sc)
>>> sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/spark-1.2.1/python/pyspark/sql.py", line 1620, in sql
    return SchemaRDD(self._ssql_ctx.sql(sqlQuery).toJavaSchemaRDD(), self)
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
  File "/usr/local/spark-1.2.1/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o17.sql.
: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:235)
	at org.apache.spark.sql.hive.HiveContext$$anonfun$4.apply(HiveContext.scala:231)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.sql.hive.HiveContext.x$3$lzycompute(HiveContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.x$3(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf$lzycompute(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveContext.hiveconf(HiveContext.scala:229)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.<init>(HiveMetastoreCatalog.scala:55)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:253)
	at org.apache.spark.sql.hive.HiveContext$$anon$4.<init>(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:263)
	at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:262)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:411)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)
	... 30 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)
	... 35 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)
	... 40 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	... 69 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85)
	... 87 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238)
	... 89 more

>>> exit()
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/bin$ cd ../conf/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ vim hive-site.xml 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ hdfs dfs -chmod -R +777 /tmp
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ cd ../../hive/conf/
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ vim hive-site.xml 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ cp hive-site.xml ../../spark-1.2.1/conf/
cp: cannot create regular file `../../spark-1.2.1/conf/hive-site.xml': Permission denied
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ sudo cp hive-site.xml ../../spark-1.2.1/conf/
hduser@arinjoy-Inspiron-3521:/usr/local/hive/conf$ cd ../../spark-1.2.1/conf/
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ vim hive-site.xml 
No protocol specified
No protocol specified
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ netstat
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
^C
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ netstat -an | grep '9083'
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ netstat -an | grep 9083
hduser@arinjoy-Inspiron-3521:/usr/local/spark-1.2.1/conf$ cd ../../hive/bin/

